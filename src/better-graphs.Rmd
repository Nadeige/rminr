---
title: "Better graphs"
author: "Paul Sharpe, Andy Wills, Chris Mitchell"
output:
  html_document:
    highlight: pygment
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and output.
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache=FALSE)
library(pander)
```

# Contents

* [Introduction](#intro)
* [Meaningful labels](#labels)
* [Journal styling](#style)
* [High-quality output](#hq)
* [Choosing a graph for your data](#choosing)
* [Within-participants manipulations](#within)
    + [One factor](#w-1x3)
    + [Two factors](#w-2x2)
* [Between-participants manipulations](#between)
    + [One factor, two levels](#b-1x2)
    + [One factor, four levels](#b-1x4)
* [Mixed (within-between) data](#mixed)
* [Pairs plot](#pairs)

<a name = "intro"></a>

# Introduction

In this worksheet, we'll look at how to produce publication-quality graphs in R. We start with an example from a previous worksheet. 

In the exercise for the [within-subject differences](anova1.html#densediff) worksheet, we produced a density plot of the within-subject differences in reaction time for congruent versus incongruent trials. Let's pick up where we left off. Go back to your `rminr-data` project, and open up the R file you used for this exercise. If you didn't complete that exercise, or can't find the file, you can get a copy [here](densediff.R).

```{r init, message=FALSE, echo=FALSE}
rm(list = ls()) # clear the environment
library(tidyverse)
words <- read_csv("wordnaming2.csv")
wordctrl <- words %>% filter(medit == "control")
wordctrlCI <- wordctrl %>% filter(congru != "neutral")
wordctrlCIsum <- wordctrlCI %>% group_by(subj, congru) %>% summarise(rt = mean(rt))
ctrl <- wordctrlCIsum %>% pivot_wider(names_from = congru, values_from = rt)
ctrldiff <- ctrl %>% mutate(diff = incong - cong)
```

After some preprocessing, the graphing command we used in that exercise was:

```{r graph}
ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red')
```

This graph looks OK, but would need some improvement before including in a report or journal article. Here are the steps of this makeover.

<a name = "labels"></a>

# Meaningful labels

The first thing to do is change the axis labels to something a bit more human readable. We use the `xlab` and `ylab` commands for this. We covered these commands previously in the [Absolute Beginners' guide](exploring-incomes.html#custom-graphs):

```{r graph2}
ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red') +
    xlab("Incongruent RT - Congruent RT (ms)") +
    ylab("Scaled density")
```

<a name = "style"></a>

# Journal styling

The default styling for `ggplot` is different to what is preferred in most psychology journals. Fortunately, we can use Tina Seabrooke's `theme_APA` to correct this. You'll find the code in the same _git_ repository as the data, so all you need to do is load in her code:

```{r source}
source("themeapa.R")
```

and then add it as a theme to your graph (much as you have used `theme_bw` in the past):

```{r graph3}
ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red') +
    xlab("Incongruent RT - Congruent RT (ms)") +
    ylab("Scaled density") +
    theme_APA
```

<a name = "hq"></a>

# High-quality output

If you are writing a report or journal article, it's generally a bad idea to screenshot your graphs, and it's also generally a bad idea to use the _export_ functionality within Rstudio. This is because both of these options produce graphics that are not high enough quality for publication.

To produce high-quality output, you should first create an object for your graph, much as we created an [object for the output of analysis](anova3.html#bfact).

```{r graph4}
dgraph <- ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red') +
    xlab("Incongruent RT - Congruent RT (ms)") +
    ylab("Scaled density") +
    theme_APA
dgraph
```

We can now use the `ggsave` command to save a high-quality version of that graph:

```{r ggsave}
ggsave(filename = "fig1.pdf", plot = dgraph, units = "cm", width = 15, height = 10)
```

## Explanation of command

`filename = "fig1.pdf"` - Save the graph as _fig1.pdf_. Try to use PDF where possible, because it produces the best quality output and the smallest file size. However, if you're unfortunate enough to be using a wordprocessor than cannot import PDF graphs (e.g. Microsoft Word) then you can use PNG format instead. You do this by changing the filename, e.g. `filename = "fig1.png"`. If you send your paper to a journal for consideration, they will also require the PDF version of your graphs as a separate attachement, as PNG files are generally not good enough for professional publications. For most internal reports (and university coursework), PNG is generally good enough.

`plot = dgraph` - Use the object called `dgraph` as the graph you want to save.

`units = "cm"` - The following commands will set the size of the graph; this command says what units these are in. You usually want to use "cm" (centimetres) but if you live in a country that hasn't adopted the metric system, you can use "in" (inches) instead. 

`width = 15` - The graph (including border etc.) should be 15 units wide (the units in this case being centimetres)

`height = 10` - The graph (including border etc.) should be 10 units high (centimetres in this case). 

## Explanation of output

A file called _fig1.pdf_ will have appeared in your _Files_ window in RStudio. You can export this in the [usual way](using-projects.html#download).

<a name="choosing"></a>

# Choosing a graph for your data

So, now you know how to create a professional-quality graph in R, but which type of graph best suits your data? A graph should visually describe patterns in data that would otherwise be difficult to communicate. Choosing and configuring the most appropriate graph for you data will depend on what you want to communicate to your reader. In practice, making this decision often involves trying out different types of graph, and the elements used to build them. You might generate new ideas for graphs after you have analysed your data and begun to interpret the results.

These subjective aspects make it difficult to provide hard and fast rules for the type of plot you should choose, and the elements that it should contain. A general piece of advice is to carefully consider the best way to represent the centre (often the mean), and distribution of your data. We present some suggestions for plotting different types of data in the following sections.

As a side note, a common criticism of student projects is that results sections don't include a graph, and appendices often contain lots of graphs that don't really contribute to the report. You can overcome this by learning to experiment with different ways of plotting your data. When you've found a graph that contributes to the argument you're trying to make, be confident and include it in your results section! Appendices are seldom read.

<a name="within"></a>

# Within-participants manipulations

We'll start by producing some graphs for within-participants manipulations.

<a name="w-1x3"></a>

## One factor

Our first example uses data from [an undergraduate student experiment on the Perruchet Effect](awdiss.html). Stay in your `rminr-data` project, create a new R file called `one-within.R`, and enter the commands that follow into that file. 

To get started on this example, we need to first load the data: 

```{r perruchet-data, message=FALSE}
lvl.sum <- read_csv('going-further/perruchet-preproc.csv')
```

The data we'll focus on here are the mean expectancy ratings (`expect`) made by each participant (`subj`) for each of three Levels of the within-subjects factor (`level`). It's not critical to understand what the within-subjects factor is here, and it would take a while to explain, but take a look at [this worksheet](awdiss.html) for full details if you are curious. The main point to appreciate is the this is a _within-subjects_ manipulation, so each participant provided data at each Level. 

Our aim is to plot a graph for this single, within-participants factor called `level`. The simplest graph we could produce here would be to just plot the three means, but, when we graph data from psychology experiments, we generally try to give an indication of the variability between participants - is everyone exactly like the mean, or do people differ? One common way of doing this it to plot some kind of 'error bar'; for example, as shown in Figure 1 of  [McAndrew et al. (2012)](https://www.researchgate.net/profile/IPL_Mclaren/publication/221752843_Dissociating_Expectancy_of_Shock_and_Changes_in_Skin_Conductance_An_Investigation_of_the_Perruchet_Effect_Using_an_Electrodermal_Paradigm/links/0a85e53c10b74f291a000000.pdf). McAndrew et al. are not clear how they calculated these bars, but it's quite likely that they are the standard errors, considering each Level separately -- because this is the most common plot of this type. Such error bars are not particularly informative because, for example, they represent the variability _between_ participants at each Level, when the experiment is a _repeated measures_ design and so it is the variability of the trends across Level that is most relevant.

In this example, we instead plot one line for each participant, and then overlay this with the means to emphasize the overall trend. Here's the final plot:

```{r perruchet, message=FALSE, echo = FALSE}
### Create a blank plot
base  <- ggplot()

### Add a line for each participant
lns  <- geom_line(aes(x=level, y=expect, group=subj, colour=subj),
                  data = lvl.sum, alpha = .25)

### Add a mean line
mdata  <- lvl.sum %>% group_by(level) %>%
  summarise(lcval = mean(lcval), expect = mean(expect))
mline  <- geom_line(aes(x=level, y=expect), data = mdata, colour="black")
mdot  <- geom_point(aes(x=level, y=expect), data = mdata, colour="black")

### Combine plots, tidy up x and y axis labels, and apply APA theme
eplot <- base + lns + mline + mdot + xlab("Level") +
    ylab("Expectancy rating") + ylim(1,5) +
    scale_x_continuous(breaks = c(1,2,3)) +
    theme_APA + theme(legend.position="none")
eplot
```

From this plot, it's clear that: (a) the overall trend is downwards, (b) most but not all participants individually show this trend, and (c) there is some variation in the absolute ratings people use.

### Building up the graph, piece by piece

This graph is a bit more complex than the ones we've produced before, but R and, in particular, `ggplot`, allows us to build up complex graphs like this piece by piece, from familiar components. The way to do this is to start with a blank page, which we do by just using the `ggplot` command on its own:

```{r blank}
### Create a blank plot
base  <- ggplot()
base
```

Next, we add a line for each participant:


```{r lines, class.source = 'numberLines lineAnchors'}
### Add a line for each participant
lns  <- geom_line(aes(x=level, y=expect, group=subj, colour=subj), 
                  data = lvl.sum, 
                  alpha = .25)
base + lns
```

In lines 2-4 above, we use the familiar `geom_line` to draw a line graph, as used previously in the [understanding interactions](anova2.html#linegraph) worksheet. The difference this time is that we set `group=subj`, giving us a different line for each participant (the participant number is in the `subj` column of the `lvl.sum` data frame). These lines are easier to distinguish from each other if they are not all the same colour, so we all set `colour=subj`, so each gets a different colour (in this case, the default is that they all get a slightly different shade of blue). The final part, `alpha = .25`, makes the lines fainter, so that the mean trend line we're going to add next is more noticeable.

In line 5, we display our new line graph on top of the background: `base + lns`. 

Next, we add an indication of the mean rating at each level. To do this, we have to calculate the means, which we can do using the `group_by` and `summarise` commands we have used in many previous worksheets:

```{r mean.trend}
### Calculate means
mdata  <- lvl.sum %>% group_by(level) %>%
  summarise(lcval = mean(lcval), expect = mean(expect))
```

Now we can use `geom_line` to plot those means:

```{r mean.trend.2}
### Plot the mean line
mline  <- geom_line(aes(x=level, y=expect), data = mdata, colour="black")
base + lns + mline
```

We can make the mean line more prominent by also including plot points on it:

```{r mean.trend.dots}
mdots  <- geom_point(aes(x=level, y=expect), data = mdata, colour="black")
base + lns + mline + mdots
```

Now, we can do various things to make the graph look nicer. First, we add `theme_APA`:

```{r apa}
### Load and add APA theme
base + lns + mline + mdots + theme_APA + theme(legend.position="none")
```

This makes the graph more in keeping with the style of graphs one normally sees in psychology journals. We also added the command `theme(legend.position="")`, which removes the legend (i.e. the key relating participant number to shade of blue, which isn't informative). 

Finally, we tidy up the x and y axis labels:

```{r apa.final}
### Tidy up x and y axis labels
base + lns + mline + mdots + theme_APA + theme(legend.position="none") +
    xlab("Level") + ylab("Expectancy rating") +
    ylim(1,5) + scale_x_continuous(breaks = c(1,2,3))
```

Most of these commands we've used in previous worksheets. The command `scale_x_continuous` allows us to set exactly what the 'tick marks' on the x-axis are going to be - it makes no sense to have a Level of e.g. 1.5 marked on the graph, because such levels do not exist in the experiment.

<a name="w-2x2"></a>

## Two factors

Our next example is from an experiment with two within-subjects factors. Participants were trained to associate two screen colours with pictures of two different food rewards. At test, they saw pairs of screen colours and food pictures. Their reaction time (RT) in milliseconds was measured in cases where the pairs matched the associations they'd previously learned (congruent trials), and in cases where the pairs did not match what they had previously learned(incongruent trials). The participants experienced this test both with, and without, a secondary task. In the secondary task, the participant had to verbally rate how much they liked pictures of faces. As all participants completed all conditions, we have a fully within-subjects design, with factors congruency (congruent, incongruent) and load (load, no load).

A two-factor within-subjects design is one of the most common in cognitive psychology. There is a fairly typical way to plot this type of data, which we've previously come across in the [understanding interactions](anova2.html#intsect) worksheet.  Specifically, we use points connected by lines to represent the means in each of the four conditions. 

We start by loading the data, keeping only the test trials, and then calculating mean reaction time for each condition for each participant, using commands we have used in several previous worksheets:

```{r w-pointline-2-preproc, message=FALSE}
raw <- read_csv('case-studies/chris-mitchell/priming-faces.csv')
priming <- raw %>%
  filter(Running == 'Test') %>%
  group_by(Subject, Congruency, Load) %>%
  summarise(RT = mean(RT, na.rm = TRUE))
```

Next we calculate the means for each condition, using now-familiar commands:

```{r cond-means}
priming.sum <- priming %>% group_by(Congruency, Load) %>%
  summarise(RT = mean(RT, na.rm = TRUE))
priming.sum
```

We can then plot the same graph as we did in the [understanding interactions](anova2.html#intsect) worksheet:

```{r w-pointline-2-1, class.source = 'numberLines lineAnchors'}
within_2x2 <- priming.sum %>%
  ggplot(aes(x=Congruency, y=RT, group=Load, colour=Load)) +
  geom_line() +
  geom_point()
within_2x2
```

### Explanation of commands

Lines 1-2 define the x axis of our plot to be the Congruency factor, and the y axis to be RT. They also instruct R to produce different lines for load and no-load (`group=Load`), and to use a different colour for these two lines (`colour=Load`) .

Line 3 plots the lines on the graph, and Line 4 adds plot points. Line 5 tells R to show the plot we have created.

### Tidying up

Finally, we apply some formatting to the graph, in the way we covered at the beginning of the current worksheet:

```{r w-pointline-2-3, message=FALSE, class.source = 'numberLines lineAnchors'}
within_2x2 +
  ylab("Mean RT") + xlab("Congruency") +
  theme_APA
```

### Explanation of commands

Line 2 adds meaningful labels to the axes. Line 3 applies our APA theme.
 
### Plotting variability

The above graph shows that, on average, the effect of congruency on RT is smaller under load. We can see this because the red line is closer to horizontal than the blue line. This is the key result of this experiment and, as in our previous example, it would be good to give our reader a sense of the variability in this key result. Does every participant show this pattern of a smaller congruency effect under load, or is there variability between participants? 

One approach we could take here is to add some kind of error bars, as we saw in the paper by [McAndrew et al. (2012)](https://www.researchgate.net/profile/IPL_Mclaren/publication/221752843_Dissociating_Expectancy_of_Shock_and_Changes_in_Skin_Conductance_An_Investigation_of_the_Perruchet_Effect_Using_an_Electrodermal_Paradigm/links/0a85e53c10b74f291a000000.pdf) in our previous example. However, as in our previous example, most people who do this use the between-subjects variability, which is largely uninformative, for the same reasons as previously discussed. We could use within-subjects confidence intervals instead, as discussed [here](http://pcl.missouri.edu/sites/default/files/morey.2008.pdf), but these are not straight forward to calculate in R, and are still less informative than the approach we use below. 

In this experiment, we're most interested in how much smaller the congruency effect is under cognitive load. We can therefore show variability in this key result by calculating a difference of differences score, and plotting the distribution of this score. This is a similar approach to the one we previously took in the [within-subject differences](anova1.html) worksheet.

So, first, we calculate the difference of differences score:

```{r w_diffdiff, class.source = 'numberLines lineAnchors'}
priming_diff <- priming %>%
  pivot_wider(names_from = c(Congruency, Load), values_from = RT) %>%
  mutate(diff = (Incongruent_NoLoad - Congruent_NoLoad) -
           (Incongruent_Load - Congruent_Load))
```

**Explanation of commands**: 

Lines 1-2 pivot the data into a wide format so we can calculate the 'difference of differences'. For each participant, lines 3-4 calculate the congruency effect under load `(Incongruent_Load - Congruent_Load)` and no load `(Incongruent_NoLoad - Congruent_NoLoad)`, then subtract the no load difference from the load difference.

**Explanation of output**:

Open `priming_diff` by clicking on it in your _Environment_. As you can see there are a range of scores in the `diff` column. On a quick inspection, just over half the participants seem to have a positive score (i.e. a bigger congruency effect in the absence of load). A positive score is the expected effect, so clearly there's a lot of variability here. 

We can visualize this variability using a density plot:  

```{r w_diff_plot}
priming_diff %>% ggplot(aes(diff)) +
  geom_density(aes(y=..scaled..)) +
  scale_x_continuous(n.breaks = 10) +
  geom_vline(xintercept = 0, colour = 'black') +
  geom_vline(xintercept = mean(priming_diff$diff), colour = 'red') +
  xlab("Congruency effect: No Load - Load RT (ms)") +
  ylab("Scaled density") +
  theme_APA 
```

**Explanation of commands**: The only command here that we haven't used before (either in this worksheet, or a previous one) is `scale_x_continuous(n.breaks = 10)`, which puts 10 'ticks' on the x axis, in this case, at every 200 ms.

**Explanation of plot**: 

The black vertical line placed at zero allows us to quickly see approximately similar numbers of participants show the expected result (a posiitve score), and its reverse (a negative score). The mean score (shown as a red vertical line) is close to zero, but positive, and we can see this positive mean is at least in part due to a few participants with very large positive scores (a long tail on the right of the distribution). 

<a name="between"></a>

# Between participants data

We now look at some graphs for between participants data.

<a name="b-1x2"></a>

## One factor, two levels

We'll plot some data from [a study in which tested children’s language development](cs-picture-naming.html). The Words in Game (WinG) task uses picture cards to test children's comprehension of, and ability to say, nouns and predicates. A predicate completes an idea about the subject of a sentence. For example, if the card showed a girl pushing a bike, the predicate would be "pushing".

In this study children were tested using one of two sets of picture cards; the set used in the Italian version of WinG, or the set used in the English version. The researchers were primarily interested in whether the children's performance differed depending on which cards were used. We can demonstrate this with plots that show the distribution of scores for the two card sets on the WinG tasks.

We start by preparing the data for plotting. These steps are described in detail in the [Better tables worksheet](better-tables.html) worksheet, so we won’t repeat the description here. Instead, we’ll just list the commands and show the final output.

```{r message=FALSE, class.source = 'numberLines lineAnchors'}
wing_preproc <- read_csv('going-further/picture-naming-preproc.csv')
wing <- wing_preproc %>%
  pivot_longer(cols = c(nc, np, pc, pp),
               names_to = 'task',
               values_to = 'correct') %>%
    select(subj, task, cards, correct)
task_names <- c(
  nc = 'Noun Comprehension',
  np = 'Noun Production',
  pc = 'Predicate Comprehension',
  pp = 'Predicate Production'  
)
wing$task <- wing$task %>% recode(!!!task_names)
wing %>% head(8) %>% pander()
```

We'll plot the data using 'half violin' plots. As the name suggests, this shows one half of a [violin plot](https://en.wikipedia.org/wiki/Violin_plot).

```{r cloud, class.source = 'numberLines lineAnchors'}
# between subjects half-violin plot
library(see)
wing %>% ggplot(aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.7, size=0) +
  xlab('WinG Task') + ylab('Accuracy (max = 20)') +
  scale_fill_grey() + theme_APA +
  theme(axis.text = element_text(size = 10))
```

### Explanation of commands

Line 2 loads the `see` package which provides the `half_violin()` function. Line 3 defines the x axis of our plot to be the WinG `task`, the y axis to be task accuracy (`correct`), and to use the `cards` factor for the fill colour. Line 4 creates a the half violin plots. `position = position_identity()` plots the two distributions on top of each other, making it easy to see how much they overlap. `alpha=0.7` changes the transparency, again to help us see the overlapping area. `size=0` removes the outline around the distributions. Line 5 gives our axes meaningful labels. Line 6 converts the fill colours to a grey pallete, and applies our APA theme. Line 7 adjusts the size of the text on the x axis, so that the task names don't overlap.

### Explanation of plot

The plot gives a visual indication of whether there were differences between the Italian and English cards on each of the tests. Given the extensive overlap in scores between the card sets, this seems unlikely.

### Why not a bar plot?

We could have used a bar plot to graph this data, but the half violin plot seemed a better choice. For example, [Newman & Scholl (2012)](https://link.springer.com/article/10.3758%2Fs13423-012-0247-5) showed that readers misinterpret bar plots, because the values within the bar are perceived as more likely than those just above, even though this is not the case.

For completeness, here's the same data plotted using bars and confidence intervals. As you can see, the code is similar to the point and line plot above.

```{r}
wing %>% ggplot(aes(x=task, y=correct, fill=cards)) +
  stat_summary(geom="bar", fun=mean, position=position_dodge(0.95)) +
  stat_summary(geom="errorbar", width = 0.1, fun.data=mean_cl_boot,
               position=position_dodge(0.95)) +
  ylab("Mean RT") + xlab("Congruency") +
  labs(caption = "Bars are 95% confidence intervals") +
  theme_APA + scale_fill_grey() +
  theme(axis.text = element_text(size = 10))
```

<a name="b-1x4"></a>

## One factor, four levels

Another common plot is the boxplot. This includes a thick line which represents the median, and a box on either side which shows how the middle 50% of the data is distributed around the median. The areas above and below the median are called the the upper and lower quartiles respectively. Together they form the 'inter-quartile range' (IQR). The 'whiskers' above and below the quartiles represent the distribution of the top and bottom 25% of the data. The dots draw attention to data points which lie outside of these ranges.

We'll create a boxplot to compare the emotional avoidance strategies for fans of Mainstream (control group), Goth, Metal and Emo music. This data came from a final year undergraduate student dissertation.

```{r avoidance, class.source = 'numberLines lineAnchors', message=FALSE}
ers_l <- read_csv('going-further/music-emotion-preproc.csv')
ers_l <- ers_l %>%
  mutate(subculture = factor(subculture,
                             levels = c('Mainstream', 'Emo', 'Goth', 'Metal')))
avoidance <- ers_l %>% filter(ers == 'avoidance')

avoidance %>% ggplot(aes(x=subculture, y=score)) +
  geom_boxplot() +
  ylab("Emotional avoidance score") + xlab("Music subculture") +
  theme_APA
```

### Explanation of commands

Line 1 loads the data. Lines 3-4 convert subculture to a factor. We use the `levels=` option to order the factor levels, making `Mainstream` first as this group was a control condition. This will make it appear on the left of our plots, so it's easy to compare against the other three subcultures. Line 5 filters out everything except the measure of emotional avoidance. Line 7 puts the avoidance `score` on the y axis and the `subculture` factor on the x axis. Line 8 creates the boxplot with its default settings. Line 9 gives the axes meaningful labels, and line 10 applies our APA theme. 

### Explanation of plot

The medians in all groups are towards the upper end of the scale. The extensive overlap between the boxes and whiskers suggests there weren't any differences between the groups. In all of the groups, a few scores fell beyond the bottom 25% of the distribution.

<a name="mixed"></a>

# Mixed (within-between) data

Mixed designs include both within-subject and between-subject variance. The main challenge here is deciding which source of variance is most useful to represent in your graph. We'll demonstrate some options using data from the student project we used earlier. This was an experiment which compared participants' emotions, before and after they listened to their preferred type of music.

```{r m-load, message=FALSE}
panas <- read_csv('going-further/music-panas.csv')
panas %>% head() %>% pander()
```

There were four groups: fans of Mainstream (control), Goth, Metal and Emo music. So the repeated measure was time (before and after listening to music), and the between-participants factor was music subculture. Emotion was measured using the 20-item Positive and Negative Affect Schedule (PANAS).

This data is in wide format, with one row per participant. We'll focus on negative affect (NA), which is in the columns `pre_na`, and `post_na`. We start bar preparing the data for our graph.
 
```{r m-pointline-1, class.source = 'numberLines lineAnchors'}
panas <- panas %>%
  mutate(subculture = factor(subculture, levels = c('Mainstream', 'Emo', 'Goth', 'Metal')))
na <- panas %>%
  pivot_longer(cols = c(pre_na, post_na),
               names_to = 'pre_post',
               values_to = 'score')
```

### Explanation of commands

Lines 1-2 make `subculture` a factor, and order the factor levels as before, so that our control condition, `Mainstream` will be on the left. The remaining lines convert the negative affect columns to long format, with a factor `pre_post` to identify when the measurement was taken. 

Was negative affect lower in any of the groups after they listened to music? We can represent this by plotting the means and the variance for the two NA measurements. As mentioned in the [More on t-tests](more-on-t.html) worksheet, when comparing two means, the 95% confidence interval is the only thing that is both useful and easy to interpret. So we'll represent the variance using error bars which represent the 95% confidence interval of each mean.

```{r point, class.source = 'numberLines lineAnchors'}
na %>%
  ggplot(aes(x=subculture, y=score, colour = pre_post, group = pre_post)) +
  stat_summary(geom="point", fun=mean, position = position_dodge(width = 0.1)) +
  stat_summary(geom="errorbar", width = 0.1, fun.data=mean_cl_boot,
               position = position_dodge(width = 0.1)) +
  ylab("NA") + xlab("Subculture") +
  scale_color_grey() + theme_APA +
  labs(caption = "Bars are 95% confidence intervals of within-group means")
```

### Explanation of commands

Lines 1-3 plot the mean points, following the same pattern as the [two factor within subjects plot](w-2x2).

Lines 4-5 add the confidence intervals to the plot. Confidence intervals are the same size on either side of the mean. To plot the bars, `ggplot()` needs information about the two sides of the interval. One approach is to write some code to calculate the interval, then pass this data to `ggplot()` to plot the lines. However, we can do this more concisely using `stat_summary()`. The option `fun.data=` is used to specify a function that returns a `y` value, and values `ymax` and `ymin` which specify an interval above and below `y`. We use the function `mean_cl_boot` to calculate a mean of `y`, and the associated confidence interval in `ymax` and `ymin`for our grouped data. The confidence interval is calculated for the factor specified by `group=` in line 2, in this case `pre_post`. `mean_cl_boot` calculates confidence intervals using [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)), which improves the interval estimate for data which isn't normally distributed.

Line 8 adds a caption, which is essential to let the reader know what the error bars represent. 

### Explanation of plot

In all groups, NA was lower after the participants listening to their preferred type of music. The non-overlapping confidence intervals for Emo suggest that this is the only group where we'll find evidence that music reduces negative affect.

Were there any differences in the effects of music on NA between groups? We can see this by plotting a difference score.

```{r m-diff, class.source = 'numberLines lineAnchors'}
na_diff <- panas %>% mutate(diff = pre_na - post_na)
na_diff %>%
  ggplot(aes(x=subculture, y=diff)) +
  stat_summary(geom="point", fun=mean) +
  stat_summary(geom="errorbar", width = 0.1, fun.data=mean_cl_boot) +
  ylab("NA difference") + xlab("Subculture") +
  labs(caption = "Bars are 95% confidence intervals of pre-post mean difference") +
  theme_APA
```

### Explanation of commands

Line 1 calculates the difference in NA after listening to music (`post_na`) by subtracting it from the baseline score (`pre_na`). Line 5 calculates a between groups confidence interval for the difference scores. The remaining lines should be familiar by now.

### Explanation of plot

The largest change was in the Emo group, the smallest was in the Metal group. The only difference appears to be between the Mainstream and Emo groups.

See [Masson & Loftus (2003)](http://personal.psu.edu/jxb14/M554/articles/Masson&Loftus2003.pdf) for a more in-depth discussion about calculating and plotting confidence intervals for within subjects, betweeen subjects, and mixed designs.

<a name="pairs"></a>

## Pairs plot

Like the correlation matrices introduced in the [Better tables](better-tables.html#cor-matrix) worksheet, we can use a 'pairs' plot to show relationships between pairs of variables.

In this plot, a set of variables are listed along the columns, and also down the rows. A pair is the cell defined by a particular row and column. Along the diagonal, variables are paired with themselves, so the correlation doesn't provide any information, as it is always 1. Above the diagonal are all of the combinations of the remaining pairs of variables. The same combinations of pairs are repated below the diagonal.

This format provides an opportunity to provide a lot of information in a small area. In this plot, we'll use some different ways of showing associations between variables, all of which will be familiar from other worksheets. In the upper part of the grid, we'll plot a [correlation coefficient](corr.html) for each pair. For the pairs in the lower part of the grid, we'll create a [scatterplot with a line of best fit](https://benwhalley.github.io/rmip/automatic-line-fitting.html). Along the diagonal, we'll create a [density plot](corr.html#sum), to show the variability for each variable.

We'll plot some data from another undergraduate dissertation. This study was interested in correlations between creative problem solving, and three other variables:

  * problems - number of problems solved
  * Openness - a common factor used in personality research
  * PsiQ - vividness of mental imagery
  * FTT - the score on a flexible thinking task

```{r pairs-2, class.source = 'numberLines lineAnchors', message=FALSE}
library(GGally, quietly = TRUE)
oit <- read_csv('going-further/openness-imagery-thinking.csv')

oit %>%
  select(psiq, openness, problems, ftt) %>%
  ggpairs(lower=list(continuous='smooth')) +
  theme_APA
```

### Explanation of commands

Line 1 loads the `GGally` package, which provides the `ggpairs()` function for creating a pairs plot. Line 2 loads the data. Line 5 selects the variables to compare. Line 6 creates the plot. For the upper area and diagaonal, we don't need to tell `ggpairs()` what to display, as the defaults are exactly what we want. This gives us a Pearson correlation coefficient for each pair above the diaganol, and a density plot for each of the four variables along the diaganol. The option `lower=list(continuous='smooth')`, tells `ggplot()` to create a scatterplot with a best-fit line for each pair in the lower area of the grid. The dots are the individual data points, the black line is the line of best fit. The grey area shows [the standard error of the line of best fit](https://benwhalley.github.io/rmip/explanations-regression.html#explanation-shaded-area-geom-smooth).

### Explanation of plot

There are small positive correlations between FTT and all other variables, and between PsiQ and Openness. There is no correlation between problems solved and either PSiQ or Openness.

The PsiQ density plot shows that nobody scored less than about 3.5, and above that value the data was normally distributed. The pattern was similar for Openness and FTT, with fewer low scores shifting the distribution slightly to the right. The number of problems solved showed the opposite pattern. There was a steady decline from people who solved only one problem, and nobody solved more than four.

Notice that the limits on the x and y scales of the density plots and scatterplots are set by the range of the data rather than the range of the scale. To keep this example simple, we haven't corrected this. For a journal article, you would use ggplot's scale adjustment functions to set the correct x and y limits for each pair.

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 


