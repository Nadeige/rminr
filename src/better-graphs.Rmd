---
title: "Better graphs"
author: "Andy Wills, Paul Sharpe"
output:
  html_document:
    highlight: pygment
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and output.
knitr::opts_chunk$set(echo = TRUE, comment=NA)
library(pander)
```

# Contents

* [Introduction](#intro)
* [Meaningful labels](#labels)
* [Journal styling](#style)
* [High-quality output](#hq)
* [Plotting principles](#principles)
* [Within participants data](#within)
    + [One factor](#w-1x3)
    + [Two factors](#w-2x2)
* [Between participants data](#between)
    + [One factor, two levels](#b-1x2)
    + [One factor, four levels](#b-1x4)
* [Mixed (within-between) data](#mixed)
    + [Two factors](#m-2x4)
* [Exploring data](#exploring)
    + [Histogram](#histogram)
    + [Pairs plot](#pairs)

<a name = "intro"></a>

# Introduction

In this worksheet, we'll look at how to produce publication-quality graphs in R, using an example from a previous worksheet.

Specifically, in the [within-subject differences](anova1.html#densediff) worksheet, we produced a density plot of the within-subject differences in reaction time for congruent versus incongruent trials

```{r init, message=FALSE, echo=FALSE}
rm(list = ls()) # clear the environment
library(tidyverse)
words <- read_csv("wordnaming2.csv")
wordctrl <- words %>% filter(medit == "control")
wordctrlCI <- wordctrl %>% filter(congru != "neutral")
wordctrlCIsum <- wordctrlCI %>% group_by(subj, congru) %>% summarise(rt = mean(rt))
ctrl <- wordctrlCIsum %>% pivot_wider(names_from = congru, values_from = rt)
ctrldiff <- ctrl %>% mutate(diff = incong - cong)
```

```{r graph}
ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red')
```

This graph looks OK, but would need some improvement before including in a report or journal article. Here are the steps of this makeover.

<a name = "labels"></a>

# Meaningful labels

The first thing to do is change the axis labels to something a bit more human readable. We use the `xlab` and `ylab` commands for this. We covered these commands previously in the [Absolute Beginners' guide](exploring-incomes.html#custom-graphs):

```{r graph2}
ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red') +
    xlab("Incongruent RT - Congruent RT (ms)") +
    ylab("Scaled density")
```

<a name = "style"></a>

# Journal styling

The default styling for `ggplot` is different to what is preferred in most psychology journals. Fortunately, we can use Tina Seabrooke's `theme_APA` to correct this. You'll find the code in the same _git_ repository as the data, so all you need to do is load in her code:

```{r source}
source("themeapa.R")
```

and then add it as a theme to your graph (much as you have used `theme_bw` in the past):

```{r graph3}
ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red') +
    xlab("Incongruent RT - Congruent RT (ms)") +
    ylab("Scaled density") +
    theme_APA
```

<a name = "hq"></a>

# High-quality output

If you are writing a report or journal article, it's generally a bad idea to screenshot your graphs, and it's also generally a bad idea to use the _export_ functionality within Rstudio. This is because both of these options produce graphics that are not high enough quality for publication.

To produce high-quality output, you should first create an object for your graph, much as we created an [object for the output of analysis](anova3.html#bfact).

```{r graph4}
dgraph <- ctrldiff %>% ggplot(aes(diff)) +
    geom_density(aes(y=..scaled..)) +
    geom_vline(xintercept = 0, colour = 'red') +
    xlab("Incongruent RT - Congruent RT (ms)") +
    ylab("Scaled density") +
    theme_APA
dgraph
```

We can now use the `ggsave` command to save a high-quality version of that graph:

```{r ggsave}
ggsave(filename = "fig1.pdf", plot = dgraph, units = "cm", width = 15, height = 10)
```

## Explanation of command

`filename = "fig1.pdf"` - Save the graph as _fig1.pdf_. Try to use PDF where possible, because it produces the best quality output and the smallest file size. However, if you're unfortunate enough to be using a wordprocessor than cannot import PDF graphs (e.g. Microsoft Word) then you can use PNG format instead. You do this by changing the filename, e.g. `filename = "fig1.png"`. If you send your paper to a journal for consideration, they will also require the PDF version of your graphs as a separate attachement, as PNG files are generally not good enough for professional publications. For most internal reports (and university coursework), PNG is generally good enough.

`plot = dgraph` - Use the object called `dgraph` as the graph you want to save.

`units = "cm"` - The following commands will set the size of the graph; this command says what units these are in. You usually want to use "cm" (centimetres) but if you live in a country that hasn't adopted the metric system, you can use "in" (inches) instead. 

`width = 15` - The graph (including border etc.) should be 15 units wide (the units in this case being centimetres)

`height = 10` - The graph (including border etc.) should be 10 units high (centimetres in this case). 

## Explanation of output

A file called _fig1.pdf_ will have appeared in your _Files_ window in RStudio. You can export this in the [usual way](using-projects.html#download).

<a name="principles"></a>

# Plotting principles

A plot should visually describe patterns in data that would otherwise be difficult to communicate. Choosing and configuring the most appropriate plot will depend on what you want to communicate to your reader. In practice, making this decision often involves trying out different types of plot, and the elements used to build them.

As such, it's difficult to provide _rules_ for ploting data. However, there are some principles which will help you to decide how to plot your data.

* central tendency
* variation
* Plot in main text (not appendices).

Barcharts or point plots sometimes include a line and crossbar either side of the mean value representing some kind of interval. If you plot interval lines, there are two essential pieces of information you should include in your caption. First, you should say what type of interval the line represents. This could be a standard deviation, a standard error or a confidence interval. We recommend plotting 95% confidence intervals, as these tend to be both useful and easy to interpret. Second, you should state whether the interval represents within or between subject variability.


In the following sections ...
examples The approach we've taken in this worksheet is to consider whether you are plotting data for within or between subjects, or a mixture of the two.

<a name="within"></a>

# Within participants data

We'll start by producing some graphs for within participants data.

<a name="w-1x3"></a>

## One factor

Our first example uses data from [an undergraduate student dissertation on the Perruchet Effect](awdiss.html). We want to plot a graph for a within participants experiment with a single factor.

```{r perruchet-data, message=FALSE, class.source = 'numberLines lineAnchors'}
lvl.sum <- read_csv('going-further/perruchet-preproc.csv')
mdata  <- lvl.sum %>% group_by(level) %>%
  summarise(lcval = mean(lcval), expect = mean(expect))
```

The simplest graph we could produce here would be to just plot the three means, but this wouldn't really add anything to the means that were presented earlier in the report. 

Generally, when we graph data from psychology experiments, we try to give an indication of the variability between participants - is everyone exactly like the mean, or do people differ? One common way of doing this it to plot some kind of 'error bar'; for example, as shown in Figure 1 of  [McAndrew et al. (2012)](https://www.researchgate.net/profile/IPL_Mclaren/publication/221752843_Dissociating_Expectancy_of_Shock_and_Changes_in_Skin_Conductance_An_Investigation_of_the_Perruchet_Effect_Using_an_Electrodermal_Paradigm/links/0a85e53c10b74f291a000000.pdf). McAndrew et al., are not clear how they calculated these bars, but it's quite likely that they are the standard errors, considering each Level separately -- because this is the most common plot of this type. Such error bars are not particularly informative because, for example, they represent the variability _between_ participants at each Level, when the experiment is a _repeated measures_ design and so it is the variability of the trends across Level that is most relevant.

In this project, we instead plot one line for each participant, and then overlay this with the means to emphasize the overall trend.

```{r perruchet, message=FALSE, class.source = 'numberLines lineAnchors'}
### Create a blank plot
base  <- ggplot()

### Add a line for each participant
lns  <- geom_line(aes(x=level, y=expect, group=subj, colour=subj),
                  data = lvl.sum, alpha = .25)

### Add a mean line
mline  <- geom_line(aes(x=level, y=expect), data = mdata, colour="black")
mdot  <- geom_point(aes(x=level, y=expect), data = mdata, colour="black")

### Combine plots, tidy up x and y axis labels, and apply APA theme
eplot <- base + lns + mline + mdot + xlab("Level") +
    ylab("Expectancy rating") + ylim(1,5) +
    scale_x_continuous(breaks = c(1,2,3)) +
    theme_APA + theme(legend.position="none")
eplot
```

### Explanation of commands

### Explanation of plot

From this plot, it's clear that: (a) the overall trend is downwards, (b) most but not all participants individually show this trend, and (c) there is some variation in the absolute ratings people use.

<a name="w-2x2"></a>

## Two factors

* be clear what type/how CI is being calculated
* Done using `stat_summary()`
* Calculate CIs using `mean_cl_boot`, as bootstrapping is a robust method that doesn't assume normally distributed data.

```{r w-pointline-2, message=FALSE}
raw <- read_csv('case-studies/chris-mitchell/priming-faces.csv')
priming <- raw %>%
   filter(Running == 'Test') %>%
   group_by(Congruency, Load)

dodge_width <- .1
priming %>% ggplot(aes(x=Congruency, y=RT, group=Load, colour=Load)) +
  stat_summary(geom="point", fun=mean, position = position_dodge(width = dodge_width)) +
  stat_summary(geom="line", fun=mean, position = position_dodge(width = dodge_width)) +
  stat_summary(geom="errorbar", width = .1, fun.data=mean_cl_boot, position = position_dodge(width = dodge_width)) +
      ylab("Mean RT") + xlab("Congruency") +
      scale_color_grey()+ theme_APA +
  labs(caption = "Bars are 95% confidence intervals")
```

### Explanation of commands


<a name="between"></a>

# Between participants data

<a name="b-1x2"></a>

## One factor, two levels

We'll plot some data from [a study in which tested children’s language development](cs-picture-naming.html). The Words in Game (WinG) task uses picture cards to test children's comprehension of, and ability to say, nouns and predicates. A predicate completes an idea about the subject of a sentence. For example, if the card showed a girl pushing a bike, the predicate would be "pushing".

In this study children were tested using one of two sets of picture cards; the set used in the Italian version of WinG, or the set used in the English version. The researchers were primarily interested in whether the children's performance differed depending on which cards were used. We can demonstrate this visually by presenting the data for the two card sets, for each of the four tasks.

We start by preparing the data for plotting. These steps are described in more detail in the [Better tables worksheet](better-tables.html) worksheet.

```{r message=FALSE, class.source = 'numberLines lineAnchors'}
wing_preproc <- read_csv('going-further/picture-naming-preproc.csv')
wing <- wing_preproc %>%
  pivot_longer(cols = c(nc, np, pc, pp),
               names_to = 'task',
               values_to = 'correct') %>%
    select(subj, task, cards, correct)
```

### Explanation of commands

Line 1 loads the data. The scores are in _wide_ format (lots of columns, few rows), but R generally requires data to be in _long_ format (lots of rows, few columns). Lines 2-5 convert the task data from wide to long. Line 6 select the columns were interested in plotting.

The first few rows of our data look like this:

```{r, echo=FALSE}
wing %>% head(8) %>% pander()
```

Next, we give the four task names more meaningful labels, in preparation for displaying them on the x-axis of our plot.

```{r mean-labels, class.source = 'numberLines lineAnchors'}
task_names <- c(
  nc = 'Noun Comprehension',
  np = 'Noun Production',
  pc = 'Predicate Comprehension',
  pp = 'Predicate Production'  
)
wing$task <- wing$task %>% recode(!!!task_names)
```

### Explanation of commands

The levels of the `task` factor currently have short names. Lines 1-6 defines a mapping between the short names and more meaningful ones. Line 7 recodes the factor to convert the short names to the more descriptive names.

We'll plot the data using 'half violin' plots. As the name suggests, this shows one half of a [violin plot](https://en.wikipedia.org/wiki/Violin_plot). We chose this type of plot as we wanted to show that distributions on some of the tasks was not normally distributed, to give the reader a feel for whether there was a difference in accuracy for any of the tasks. This paved the way for [the Mann-Whitney U tests in the next section of the results](cs-picture-naming.html#mann-whitney).

```{r cloud, class.source = 'numberLines lineAnchors'}
# between subjects half-violin plot
library(see)
wing %>% ggplot(aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.7, size=0) +
  scale_fill_grey() +
  theme_APA +
  theme(axis.text = element_text(size = 10)) +
  xlab('WinG Task') + ylab('Accuracy (max = 20)')
```


There is evidence that readers misinterpret bar plots, because we percieve values within the bar as more likely than those just above, even though this is not the case (Newman & Scholl, 2012).

```{r}
wing %>% ggplot(aes(x=task, y=correct, fill=cards)) +
  stat_summary(geom="bar", fun=mean, position=position_dodge(0.95)) +
  stat_summary(geom="errorbar", width = .1, fun.data=mean_cl_boot, position=position_dodge(0.95)) +
      ylab("Mean RT") + xlab("Congruency") +
      theme_APA + scale_fill_grey() +
    theme(axis.text = element_text(size = 10)) +
  labs(caption = "Bars are 95% confidence intervals")
```

<a name="b-1x4"></a>

## One factor, four levels

Boxplot

```{r avoidance}
ers_l <- read_csv('going-further/music-emotion-preproc.csv')
ers_l <- ers_l %>% mutate(subculture = factor(subculture, levels = c('Mainstream', 'Emo', 'Goth', 'Metal')))

avoidance <- ers_l %>% filter(ers == 'avoidance')
avoidance %>% ggplot(aes(subculture, score)) + geom_boxplot() +
  theme_APA
```

**Explanation of commands:**

**Explanation of output:**


### Explanation of commands

Line 1 loads the `see` package which provides the `half_violin()` function. Line 2 defines the x axis of our plot to be the WinG tasks, the y axis to be task accuracy (1-20), and to use the `cards` factor for the fill colour. Line 3 creates the half violin plot. `position = position_identity()` plots the two distributions on top of each other, making it easy to see how much they overlap. `alpha=0.7` changes to transparency, again to help us see the overlapping area. `size=0` removes the outline around the distributions. Line 4 uses a grey palette for filling in the two distributions. Line 5 gives our axes meaningful labels.

**Explanation of commands:**

Lines 1-6 recode `task` factor levels, to make them more meaningful on the plot's x axis. Line 7 loads the `see` package which provides the `half_violin()` function. Line 8 defines the x axis of our plot to be the WinG tasks, the y axis to be task accuracy (1-20), and to use the `cards` factor for the fill colour. Line 9 creates a "half violin" plot. As the name suggests, this shows one half of a [violin plot](https://en.wikipedia.org/wiki/Violin_plot). `position = position_identity()` plots the two distributions on top of each other, making it easy to see how much they overlap. `alpha=0.3` changes to transparency, again to help us see the overlapping area. `size=0` removes the outline around the distributions. Line 10 gives our axes meaningful labels.

**Explanation of output:**

This plot gives a visual indication of whether there were differences between the Italian and English cards on each of the tests. Given the extensive overlap in scores between the card sets, this seems unlikely. The plot also shows that the data were slightly skewed on the noun tasks due to some low scores.

<a name="mixed"></a>

# Mixed (within-between) data

<a name="m-2x4"></a>

## Two factors

The problem is of course, within and between subject variance.

FIXME: No error bars, or error bars for the factor of interest.

This could be what we want to show.

```{r m-pointline-1, message=FALSE}
panas <- read_csv('going-further/music-panas.csv')
panas <- panas %>% mutate(subculture = factor(subculture, levels = c('Mainstream', 'Emo', 'Goth', 'Metal')))
na <- panas %>%
  pivot_longer(cols = c(pre_na, post_na),
               names_to = 'pre_post',
               values_to = 'score')
```

```{r point}
na %>%
    ggplot(aes(x=subculture, y=score, colour = pre_post, group = pre_post)) +
        stat_summary(geom="point", fun=mean, position = position_dodge(width = dodge_width)) +
        stat_summary(geom="errorbar", width = .1, fun.data=mean_cl_boot, position = position_dodge(width = dodge_width)) +
      ylab("Negative affect") + xlab("Subculture") +
    scale_color_grey() + theme_APA +
  labs(caption = "Bars are 95% confidence intervals of subculture means")
```

However, it's more likely that we're interested in the difference in NA before and after listening to the music.

```{r}
na_diff <- panas %>% mutate(diff = pre_na - post_na)
na_diff %>%
    ggplot(aes(x=subculture, y=diff)) +
        stat_summary(geom="point", fun=mean) +
        stat_summary(geom="errorbar", width = .1, fun.data=mean_cl_boot) +
      ylab("NA difference (pre-post)") + xlab("Subculture") +
  theme_APA + theme(plot.caption = element_text(size=8)) +
  labs(caption = "Bars are 95% confidence intervals of mean differences")
```

<a name="exploring"></a>

# Exploring data

<a name="histogram"></a>

## Histogram

[one of the first graphs](#graphs)

```{r}
oit <- read_csv('going-further/openness-imagery-thinking.csv')

n_obs = sum(!is.na(oit$problems))
bw = 1
mean <- mean(oit$problems)
sd <- sd(oit$problems)

oit %>% ggplot(aes(problems))  + 
  geom_histogram(colour = "black", binwidth = bw) + 
  stat_function(fun = function(x) 
    dnorm(x, mean = mean, sd = sd) * bw * n_obs) +
  theme_APA +
  xlab('Problems solved') + ylab('Count (participants)')
```

**Explanation of commands:**

**Explanation of output:**

<a name="pairs"></a>

## Pairs plot

* PsiQ total, Openness, problem scores, FTT

```{r, message=FALSE}
library(GGally, quietly = TRUE)
library(ggplot2)
lowerFn <- function(data, mapping, ...) {
   ggplot(data = data, mapping = mapping) +
      geom_point(size=1) +
      geom_smooth(color = 'black', method='lm', size=.5)
}
oit %>%
   select(psiq, openness, problems, ftt) %>%
   ggpairs(lower=list(continuous=wrap(lowerFn))) +
  theme_APA
```

**Explanation of commands:**

**Explanation of output:**

# References

Newman, G. E., & Scholl, B. J. (2012). [Bar graphs depicting averages are perceptually misinterpreted: The within-the-bar bias](https://link.springer.com/article/10.3758%2Fs13423-012-0247-5) Psychonomic Bulletin & Review, 19(4), 601–607.

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 


