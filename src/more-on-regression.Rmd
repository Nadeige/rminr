---
title: "More on regression"
author: "Paul Sharpe and Andy Wills"
output:
  html_document:
    highlight: pygment
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate
## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)
## Show commands and output.
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache = TRUE)
library(pander)
# Line numbering guide
# https://blog.atusy.net/submodules/rmd-line-num/index.html
```

## Contents

- [Introduction](#intro)

- [Getting started](#start)

- [Multiple regression with more than two predictors](#multi)

- [Hierarchical regression](#hierarchical)

- [Exercise 1](#ex1)

- [Exercise 2](#ex2)

<a name="intro"></a>

## Introduction

Previous worksheets introduced [linear regression using a single predictor variable](https://benwhalley.github.io/rmip/regression.html), and [multiple regression with two predictors](https://benwhalley.github.io/rmip/multiple-regression.html). This worksheet builds on this foundation, by explaining how to build models with more than two predictors.

<a name="start"></a>

## Loading data

Open the `rminr-data` project we used [previously](preproc.html#load).

Ensure you have the latest files by asking git to "`pull`" the repository. Select the `Git` tab, which is located in the row of tabs which includes the `Environment` tab. Click the `Pull` button with a downward pointing arrow. A window will open showing the files which have been pulled from the repository. Close the `Git pull` window. The `going-further` folder should contain the file `religion.csv`.

Next, create a new, empty R script and save it in the `rminr-data` folder as `more-regression.R`. Put all the commands from this worksheet into this file, and run them from there. Save your script regularly.

We'll use some data from an undergraduate student dissertation which explored relationships between Polish (n=93) and British (n=104) people's religious orientation, spiritual beliefs and emotional intelligence. We start by loading the data.

```{r load, message=FALSE}
rm(list = ls()) # clear the environment
library(tidyverse)
data <- read_csv('going-further/religion-preproc.csv')
```

The following sections describe the columns in `data`.

### Demographics

| Column | Description                             | Values             |
| ------ | --------------------------------------- | ------------------ |
| subj   | Unique anonymous participant number     | 1-197              | 
| age    | Age of participant                      | 18-74              |
| sex    | Biological sex of participant           | male, female       |
| education | Highest education level of participant  | no formal quals, GCSE or equiv, A level or equiv, degree, Technical, HNC, FD or equiv |
| religious  | Did the participant identify with a recognised religion? | yes, no  |
| nationality | Nationality of participant         | British, Polish |

We need to do a little bit of preprocessing on the demographics data.

```{r demographics, class.source = 'numberLines lineAnchors', warning=FALSE}
edu_order <- c('no formal quals', 'GCSE or equiv', 'A level or equiv',
               'Technical, HNC, FD or equiv', 'degree')
data <- data %>% mutate(sex = factor(sex),
                        education = factor(education, levels = edu_order))
levels(data$education)
```

**Explanation of commands:**

Lines 1-2 create a vector (a list) of education levels in ascending order. Lines 3-4 convert the `sex` and `education` columns to factors. Because the order of factor levels matters for education, we use `levels = edu_order` to order them. If we didn't do this, the levels would have been placed in alphabetical order, with `A level or equiv` lowest, and `Technical, HNC, FD or equiv` highest. Line 5 displays the order `education` levels.

**Explanation of output:**

The `education` factor levels are now in ascending order, which makes sense for using them as a predictor variable in a regression model. 

We now turn to the three scales that the participants completed. In the the data we loaded, the scale scores have already been calculated. In practice, you would need to calculate the scores for each scale from the raw data. This is explained in the [Preprocessing scales worksheet](preproc-scales.html).

### Religions Orientation
 
Religious orientation (RO) was was measured with the amended Religious Orientation Scale (ROS). According to the ROS, people who are intrinsically religious treat religion as a spiritual end in and of itself. Those who are extrinsically religious practise religion for self-serving reasons, such as social status. Therefore, the ROS has two subscales. 

| Column | Description                    | Values |
| ------ | ------------------------------ | ------ |
| ro_i   | Intrinsic relgious orientation | 1-3    | 
| ro_e   | Extrinsic relgious orientation | 1-3    |

### Spirituality

Spirituality was measured with the Spiritual Connection Questionnaire (SCQ). The SCQ has five subscales: connection with nature, connection with places, connection with the universe, connection with other people, and .

| Column | Description                                                            | Values |
| ------ | ---------------------------------------------------------------------- | ------ |
| happiness | Extent to which spirituality brings the participant happiness | 1-7| 
| places   | Extent to which the participant feels spiritually connected to places | 1-7 |
| others | Extent to which the participant feels spiritually connected to others | 1-7  |
| nature | Extent to which the participant feels spiritually connected to nature | 1-7 |
| universe | Extent to which the participant feels spiritually connected to the universe | 1-7 |

### Trait Emotional Intelligence

In this study, emotional intelligence was treated as a trait; a personality factor relating to various aspects of emotions. This was measured using the Short Form Trait Emotional Intelligence Questionnaire (TEIQue-SF). The TEIQue-SF has four subscales: wellbeing, self-control, emotionality and sociability.

| Column | Description                                                            | Values |
| ------ | ---------------------------------------------------------------------- | ------ |
| tei    | Total trait emotional intelligence score                               | 1-7    |
| wellbeing | Wellbeing | 1-7| 
| self_control   | Self-control | 1-7 |
| emotionality | Emotionality | 1-7  |
| sociability | Sociability | 1-7 |

<a name="multi"></a>

## Multiple regression with more than two predictors

The linear model you built in [the multiple regression worksheet](https://benwhalley.github.io/rmip/multiple-regression.html) worksheet used two predictors. It's it's straightforward to add additional predictors to a model. We'll start with a model which predicts intrinsic religious orientation from the demographics variables in our Polish sample.

```{r oi1, class.source = 'numberLines lineAnchors', warning=FALSE}
polish <- data %>% filter(nationality == 'Polish')
polish <- polish %>% drop_na() %>% as.data.frame()

oi_lm1 <- lm(ro_i ~ age + sex + education + religious, data = data.frame(polish))
library(broom)
glance(oi_lm1)
```

**Explanation of commands:**

Line 1 filters the data to only include Polish participants. We'll be using Bayesian models to test our hypotheses, as described in [the multiple regression worksheet](https://benwhalley.github.io/rmip/how-good-was-our-model.html). We can't build these models for predictors which don't have a value, so we use `drop_na()` to remove any rows containing `NA`. Line 2 also converts `polish` to a data frame, as this is the format required for calculating Bayes Factors.

Line 4 builds a regression model that predicts intrinsic religious orientation, defined by `ro_i ~`. The variables on the right of the `~` are the predictors. As you can see, these are the four demographics variables `age + sex + education + religious`. Line 5 loads the `broom` library which provides the `glance()` function. Line 6 uses `glance()` to print some statistics for our model.

**Explanation of output:**

```{r oi_lm1-hidden, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
r_squared1 <- round(glance(oi_lm1)$adj.r.squared, 2)
```

As explained in previous worksheets, the value of `r r_squared1` in `adj.r.squared` tells us how much of the variance in `ro_i` is explained by the demographics variables. A model containing demographics variables explain `r r_squared1 * 100`% of the variance in intrinsic religious orientation.

[The multiple regression worksheet](https://benwhalley.github.io/rmip/how-good-was-our-model.html) also showed you how to calculate a Bayes factor to decide whether a regression model is any better than a simpler model with no predictors.

```{r oi1-bf, class.source = 'numberLines lineAnchors', warning=FALSE}
library(BayesFactor, quietly = TRUE)
oi_lmbf1 <- lmBF(ro_i ~ age + sex + education + religious, data = data.frame(polish),
                 progress = FALSE)
oi_lmbf1
```

**Explanation of commands:**

Line 1 loads the `BayesFactor` package. Lines 2-3 build a Bayesian model using the same outcome and predictors as above. Line 4 displays information about the model.

**Explanation of output:**

The output `[1] age + sex + education + religious` reminds you of the predictors in your model. The number after the `:` is a Bayes Factor for they hypothesis that your model is a better predictor of the outcome than a simpler model with no predictors. That is, a model which just computes the average of all outcome scores (sometimes called the 'Intercept only' model). The Bayes Factor for this hypothesis is around 3,800, which is strong evidence that these demographics variables help predict intrinsic religiosity.

<a name="hierarchical"></a>

## Hierarchical regression

One use of multiple regression is to test a sequence of related hypotheses. Groups of variables (sometimes referred to as 'blocks') are added to a model in steps. At each step, a comparison is made to see if the model with more variables explains more or less variance than the simpler model from the previous step.

The order in which variables are added to the model is not arbitrary. Based on previous research, variables known to predict the outcome are entered first. Variables which test new hypotheses are added in subsequent steps. Each step builds on the previous one, which is why the approach is known as 'hierarchical regression'.

So far, we have a model which shows that demographics help to predict intrinsic religiosity. According to the definition we gave above, intrinsically religious people treat religion as a spiritual end in and of itself. If that's true, we would expect a model that includes spirituality predictor variables to be better than one with just the demographics variables. Again, by 'better', we mean 'explain more variance' in the outcome variable.

We can test this hypothesis by extending the demographics model to include the spirituality variables measured by the SCQ.

```{r oi2, class.source = 'numberLines lineAnchors', warning=FALSE}
oi_lm2 <- lm(ro_i ~ age + sex + education + religious +
               happiness + universe + others + nature + places,
             data = data.frame(polish))
glance(oi_lm2)
```

**Explanation of commands:**

Lines 1-3 build our new model. This includes the demographics variables, and adds the spirituality variables `happiness + universe + others + nature + places`.

**Explanation of output:**

```{r oi_lm2-hidden, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
r_squared2 <- round(glance(oi_lm2)$adj.r.squared, 2)
```

For this model, `adj.r.squared` = `r r_squared2`, which means it explains `r r_squared2 * 100`% of the variance in intrinsic religious orientation, an increase of `r (r_squared2 - r_squared1) * 100`% over the model from step 1.

In the previous step, we found that a model with demographics predictors was better than a model with no predictors. To test the hypothesis that the spirituality variables help to explain the variance in intrinsic religiosity, we can compare our second model against the first. This approach was explained in the [multiple regression worksheet](https://benwhalley.github.io/rmip/how-good-was-our-model.html), and you also encountered it when [testing the interaction in a factorial ANOVA](anova4.html#twoBS).

```{r oi2-bf, class.source = 'numberLines lineAnchors', warning=FALSE}
oi_lmbf2 <- lmBF(ro_i ~ age + sex + education + religious +
               happiness + universe + others + nature + places,
               data = data.frame(polish), progress = FALSE)
oi_lmbf2 / oi_lmbf1
```

**Explanation of commands:**

Lines 1-3 build a Bayesian regression model with the demographic and spirituality predictors. Line 4 compares the model with the demographics and spirituality variables, `oi_lmbf2`, against the model with just the demographics variables, `oi_lmbf1`. You can think of this as dividing a more complex model by a simpler one, but remember that this is actually a comparison between the probabilites of the two models.

**Explanation of output:**

The Bayes Factor for this hypothesis is over 31 million, so we can confidently claim that a model containing these spirituality variables explains more variance in intrinsic religiosity, than a simpler model with just demographics variables.

In this study, the researchers also hypothesised that trait emotional intelligence and extrinsic religiosity would further explain the variance in intrinsic religiosity. These variables were entered together as a block.

```{r oi3, class.source = 'numberLines lineAnchors', warning=FALSE}
oi_lm3 <- lm(ro_i ~ age + sex + education + religious +
               happiness + universe + others + nature + places +
               wellbeing + self_control + emotionality + sociability + ro_e,
             data = data.frame(polish))
glance(oi_lm3)
```

**Explanation of commands:**

Lines 1-4 build our third model. This contains all of the variables in `oi_lmbf2`, and adds the emotional intelligence variables `wellbeing + self_control + emotionality + sociability`, and `ro_e` for extrinsic religiosity.

**Explanation of output:**

```{r oi_lm3-hidden, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
r_squared3 <- round(glance(oi_lm3)$adj.r.squared, 2)
```

For this model, `adj.r.squared` = `r r_squared3`, which means it explains `r r_squared3 * 100`% of the variance in intrinsic religious orientation, an increase of `r (r_squared3 - r_squared2) * 100`% over the model from step 2.

Finally, we compare the Bayesian model, with the one from step 2.

```{r oi3-bf, class.source = 'numberLines lineAnchors', warning=FALSE}
oi_lmbf3 <- lmBF(ro_i ~ age + sex + education + religious +
               happiness + universe + others + nature + places +
               wellbeing + self_control + emotionality + sociability + ro_e,
               data = data.frame(polish), progress = FALSE)
oi_lmbf3 / oi_lmbf2
```

**Explanation of commands:**

Lines 1-4 build the same Bayesian model. Line 5 compares this new model against the previous one.

**Explanation of output:**

The Bayes Factor for this hypothesis is over 11,000, which is strong evidence that a model with the additional variables explains more variance than the simpler model in step 2.

### A note about 'overfitting'

All of the regression models we've been creating and comparing were built based on the data from our sample. However, we're normally looking for a model that will predict outcomes for _any_ sample. Adding variables to a model can explain more variance in a particular sample, at the expense of being able to explain data in other samples. This is known as [overfitting](https://en.wikipedia.org/wiki/Overfitting). As a general principle, we can reduce the risk of overfitting by preferring a simpler model over a more complex one, when adding variables doesn't increase _R^2^_ by much. Other methods, such as [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), provide more precise tests of overfitting, but these are outside the scope of this worksheet.

<a name="ex1"></a>

## Exercise 1

Build a model which predicts trait emotional intelligence from the demographics variables. The adjusted _R^2^_ and Bayes Factor should look like this:

```{r ei1, echo=FALSE}
british <- data %>% filter(nationality == 'British') %>%
  drop_na() %>% # ro_e has 4 NAs
  as.data.frame()

ei_lm1 <- lm(tei ~ age + sex + education + religious, data = data.frame(british))
glance(ei_lm1)

ei_bf1 <- lmBF(tei ~ age + sex + education + religious, data = data.frame(british),
            progress = FALSE)
ei_bf1
```

**Copy the R code you used for this exercise into PsycEL.**

<a name="ex2"></a>

## Exercise 2

Build a second model which predicts trait emotional intelligence from the demographics and spirituality variables. Compare this against the model you built in the previous exercise. The adjusted _R^2^_ and Bayes Factor should look like this:

```{r ei2, echo=FALSE}
ei_lm2 <- lm(tei ~ age + sex + education + religious +
               happiness + universe + others + nature + places,
             data = data.frame(british))
glance(ei_lm2)

ei_bf2 <- lmBF(tei ~ age + sex + education + religious  +
               happiness + universe + others + nature + places,
               data = data.frame(british), progress = FALSE)
ei_bf2 / ei_bf1
```

**Copy the R code you used for this exercise into PsycEL.**

**Write a sentence interpreting _R^2^_ and the Bayes Factor for the model comparison.**

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0.