---
title: "Reliability of Targets in a Picture Naming Task"
author: "Allegra Cattani, Adele Conn, Paul Sharpe"
output: html_document
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Data required to knit
## https://github.com/ajwills72/rminr-data/tree/master/going-further/picture-naming.csv
##
## I check out rminr-data and make a symbolic link to going-further

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and output.
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache = TRUE)
```

# Contents

- [Introduction](#intro)

- [Getting started](#start)

- [Preprocessing](#preprocessing)

- [Results](#results)

<a name="intro"></a>

# Introduction

This study was an experiment which evaluated Words in Game (WinG) test, a tool for assessing childrenâ€™s language development. WinG consists of a set of picture cards which are used in four tests: noun comprehension, noun production, predicate comprehension, and predicate production. The Italian and English versions of the WinG cards use different pictures to depict the associated words. The experiment tested whether English-speaking children aged approximately 30 months, produce similar responses for the two sets of cards. This was a 4 (test) x 2 (cards) mixed design.

The data for each test was recorded on a sub-sheet of an Excel spreadsheet. There was also a sub-sheet containing demographic data.

<a name="start"></a>

# Getting started

To prepare for this worksheet, follow these steps:

1. Open the project you used to complete the [Better scales](better-scales.html) worksheet.

<a name="preprocessing"></a>

# Preprocessing

Relevant worksheets: [Better tables](#better-tables.html).

We summarise the preprocessing steps here. They explained in full in the [Better tables](#better-tables.html) worksheet.

We start by reading the data from `picture-naming.xlsx` and preprocessing the demographics sub-sheet. This includes scores for the Communicative Development Inventory (CDI). The CDI is a list of 100 words with columns 'understands' and 'says'. For each word, parents place a tick in these columns if they think their child can understand and/or say the word. The scores for 'understands' (`cdi_u`) and 'says' (`cdi_s`) is a total of the ticked boxes in the two columns.

```{r demographics, message=FALSE}
rm(list = ls()) # clear the environment
library(tidyverse)
library(pander)

# read data
library(readxl)
path <- 'going-further/picture-naming.xlsx'
data <- path %>%
  excel_sheets() %>%
  .[-1] %>%
  set_names() %>%
  map_df(~ read_excel(path = path, sheet = .x, range = "A1:V20"), .id = "sheet")

# demographics data
demographics <- data %>%
  filter(sheet == 'Demographic') %>%
  set_names(~ str_to_lower(.)) %>%
  select(gender, cdi_u, cdi_s) %>%
  mutate(gender = factor(gender), subj = factor(seq.int(nrow(.))))
demographics$gender <- recode_factor(demographics$gender, Male = 'male', Female = 'female')
demographics %>% head(3) %>% pander()
```

Next we preprocess the WinG data. When the experimenters administer WinG, they use a textual coding scheme to record each child's responses. The codes we'll use for the analyses in this worksheet are:

* `N/A` (not to be confused with the `R` data type `NA`) - researchers use this code to indicate that there was some kind of interruption to the task (e.g. the child began crying).
* `C` or `C*` - `C` indicates that the child responded correctly for the picture on the card, `C*` indicates that the response was a correct synonym. In this experiment, both of these values are considered correct responses.

The `exclude()` function applies the following exclusion criteria to our data:

1. Any subject 'N/A' for one of the first 17 words in a task. This criterion excludes children with insufficient data for analysis for a task.
1. Any subject in the remaining data whose accuracy was less than two standard deviations below the mean. This is a more general criterion to exlude subjects who had especially low scores for various reasons.

`exclude()` also calculates the total score for each WinG task (noun and predicate comprehension and production).

```{r WinG}
# task data
nc <- data %>%
  filter(sheet == 'Noun Comprehension') %>%
  set_names(~ str_to_lower(.)) %>%
  select(mountain:wellyboots, cards) %>%
  mutate(subj = factor(seq.int(nrow(.))))
nc %>% head(3) %>% pander(split.table = Inf)

cards <- nc %>%
  select(subj, cards) %>%
  mutate(cards = recode_factor(.$cards, `1` = 'italian', `2` = 'english'))
cards %>% pander()

# apply exclusion criteria to sub-test data
exclude <- function(df) {
  # exclude if N/A in item 17 or lower
  logical_matrix <- df == 'N/A'
  q17 <- logical_matrix %>%
    which(arr.ind = TRUE) %>%
    data.frame() %>%
    group_by(row) %>%
    summarise(min = min(col)) %>%
    mutate(subj = factor(row)) %>%
    select(subj, min)
  q17 <- left_join(df, q17, by='subj') %>%
    replace_na(list(min = 20))
  q17 <- q17 %>% filter(min > 17) %>% select(-min)
  
  # calculate total correct
  q17 <- q17 %>% mutate(correct = rowSums(. == 'C' | . == 'C*'))
  
  # exclude participants with scores < 2 sd below the mean
  q17 %>% filter(correct < mean(correct) + 2 * sd(correct))
}

nc <- exclude(nc)
nc_by_subj <- select(nc, subj, correct)
nc_by_subj %>% pander()

np <- data %>%
  filter(sheet == 'Noun Production') %>%
  set_names(~ str_to_lower(.)) %>%
  select(cards, beach:gloves) %>%
  mutate(subj = factor(seq.int(nrow(.))))

np <- exclude(np)
np_by_subj <- np %>% select(subj, correct)

pc <- data %>%
  filter(sheet == 'Predicate Comprehension') %>%
  set_names(~ str_to_lower(.)) %>%
  select(cards, big:pulling) %>%
  mutate(subj = factor(seq.int(nrow(.))))
pc <- exclude(pc)

pc_by_subj <- pc %>% select(subj, correct)

pp <- data %>%
  filter(sheet == 'Predicate Production') %>%
  set_names(~ str_to_lower(.)) %>%
  select(cards, small:pushing) %>%
  mutate(subj = factor(seq.int(nrow(.))))
pp <- exclude(pp)

pp_by_subj <- pp %>% select(subj, correct)

# join data
task_by_subj <- left_join(demographics, nc_by_subj, by='subj') %>%
  left_join(., np_by_subj, by='subj', suffix = c('_nc','_np')) %>%
  left_join(., pc_by_subj, by='subj') %>%
  left_join(., pp_by_subj, by='subj', suffix = c('_pc', '_pp')) %>%
  left_join(., cards, by='subj') %>%
  mutate(nc = correct_nc, np = correct_np, pc = correct_pc, pp = correct_pp) %>%
  select(subj, gender, nc, np, pc, pp, cards, cdi_u, cdi_s)
task_by_subj <- as.data.frame(task_by_subj)
```

Our preprocessed data, `task_by_subj`, includes a subject number (`subj`), the child's `gender`, their WinG scores (or `NA` if they were excluded from one of the tests, `nc`, `np`, `pc`, `pp`), the `cards` they were tested with, and their parents' CDI 'understands' (`cdi_u`) and 'says' (`cdi_s`) ratings.

```{r echo=FALSE}
task_by_subj %>% pander()
```

<a name="results"></a>

# Results

## Compare language ability between groups

Relevant worksheets: [Evidence](evidence.html)

For our first analysis, we want to check that there were no differences between the children assigned to the Italian and English card groups. We do this using t-tests of their parents' CDI ratings.

```{r cdi}
library(BayesFactor, quietly=TRUE)
cdi_u_bf <- ttestBF(formula=cdi_u ~ cards, data = task_by_subj)
cdi_u_bf
cdi_s_bf <- ttestBF(formula=cdi_s ~ cards, data = task_by_subj)
cdi_s_bf
```

```{r eval=FALSE}
# NOTE: Their t(22) is becuause 6 @ 24m + 16 @ 30m = 22
```

Our Bayes factors (CDI understands = `0.40`; CDI says = `0.45`) do not provide evidence of differences between the children tested using the Italian and English cards, giving us some confidence that the two groups are matched on language ability.

## Descriptive statistics

Relevant worksheets: [Better tables](#better-tables.html)

Next we produce a table of descriptive statistics. We start by calculating means and standard deviations for each WinG task by gender.

```{r descriptives}
task_by_subj_l <- task_by_subj %>%
  pivot_longer(cols = c(nc, np, pc, pp),
               names_to = 'task',
               values_to = 'correct')

task_descriptives <- function(df, y) {
  df %>%
    summarise(mean = mean(correct, na.rm = TRUE),
              sd = sd(correct, na.rm = TRUE),
              n = sum(! is.na(correct))) %>%
    mutate_if(is.numeric, round, 2)
}

descriptives <- task_by_subj_l %>%
  group_by(task, gender) %>%
  group_modify(task_descriptives) %>%
  ungroup()

descriptives <- descriptives %>%
  select(-n) %>%
  pivot_wider(names_from = gender, values_from = c(mean, sd)) %>%
  select(task, mean_male, sd_male, mean_female, sd_female)

descriptives %>% pander()
```

We'll add to this data frame some t-tests comparing accuracy by gender for each WinG test.

```{r WinG-t-by-gender}
# t-test for accuracy by gender
t_gender <- function(df, group) {
  df <- drop_na(df) # remove data for excluded subjects
  bf <- ttestBF(formula=correct ~ gender, data = data.frame(df))
  extractBF(bf)
}

gender_bf <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(t_gender) %>%
  mutate(bf = round(bf,2)) %>%
  select(task, bf)

descriptives <- left_join(descriptives, gender_bf, by = 'task')

descriptives %>% pander()
```

We tidy up the data and then present the data frame as a table.

```{r descriptives-presentation}
descriptives <- descriptives %>%
  unite("male", mean_male:sd_male, sep=' (') %>%
  unite("female", mean_female:sd_female, sep=' (') %>%
  mutate(male = paste0(male, ')'), female = paste0(female, ')'), task = factor(task),
         task = recode_factor(.$task, nc = 'Noun Comprehension', np = 'Noun Production',
                              pc = 'Predicate Comprehension', pp = 'Predicate Production'))

library(kableExtra)
descriptives %>% kable(
  col.names = c('Task', 'Male', 'Female', 'Bayes Factor')) %>%
  kable_styling()
```

The Bayes factors provide no evidence of differences in accuracy between male and female children, suggesting that test scores are not influenced by gender differences.

## Correlations between CDI and WinG

Relevant worksheets: [Relationships, part 2](corr.html)

Next we look at correlations between parents' ratings of their child's comprehension (CDI understands) and the child's comprehension scores on the noun and predicate tests. We use the `spearman` method of calculating the correlation between `task_by_subj$cdi_u` and `task_by_subj$nc`. The option `use="complete.obs"` deletes any cases with a value of `NA`. We set cells to `NA` above to indicate that a subject was excluded from a test. 

```{r cdi_u_correlations}
cor(method = 'spearman', task_by_subj$cdi_u, task_by_subj$nc, use="complete.obs")
correlationBF(task_by_subj$cdi_u, task_by_subj$nc)

cor(method = 'spearman', task_by_subj$cdi_u, task_by_subj$pc, use="complete.obs")
correlationBF(task_by_subj$cdi_u, task_by_subj$pc)
```

The Bayes factors for correlations of 0.70 for noun comprehension, and 0.55 for predicate comprehension do not provide evidence for a correlation between the childrens' comprehension scores and their parents' ratings of their comprehension. 

We look at similar correlations between parents' ratings of their child's ability to say words (CDI says), and the childrens' noun and predicate production scores.

```{r cdi_s_correlations}
cor(method = 'spearman', task_by_subj$cdi_s, task_by_subj$np, use="complete.obs")
correlationBF(task_by_subj$cdi_s, task_by_subj$np)

cor(method = 'spearman', task_by_subj$cdi_s, task_by_subj$pp, use="complete.obs")
correlationBF(task_by_subj$cdi_s, task_by_subj$pp)
```

Here there was a strong positive correlation (_r_ = 0.57, _BF_ = 5.09) between the parents' ratings of the words their children could say and the children's scores on the noun production test. There was no evidence for a correlation between CDI says and predicate production scores (_BF_ = 0.59).

## Comparison of Italian and English cards

Relevant worksheets:  [Better graphs](better-graphs.html), [Traditional non-parametric tests](non-parametric.html).

```{r flat-violin, echo=FALSE}
"%||%" <- function(a, b) {
  if (!is.null(a)) a else b
}

geom_flat_violin <- function(mapping = NULL, data = NULL, stat = "ydensity",
                             position = "dodge", trim = TRUE, scale = "area",
                             show.legend = NA, inherit.aes = TRUE, ...) {
  layer(
    data = data,
    mapping = mapping,
    stat = stat,
    geom = GeomFlatViolin,
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      trim = trim,
      scale = scale,
      ...
    )
  )
}

GeomFlatViolin <-
  ggproto("GeomFlatViolin", Geom,
    setup_data = function(data, params) {
      data$width <- data$width %||%
        params$width %||% (resolution(data$x, FALSE) * 0.9)

      # ymin, ymax, xmin, and xmax define the bounding rectangle for each group
      data %>%
        group_by(group) %>%
        mutate(
          ymin = min(y),
          ymax = max(y),
          xmin = x,
          xmax = x + width / 2
        )
    },

    draw_group = function(data, panel_scales, coord) {
      # Find the points for the line to go all the way around
      data <- transform(data,
        xminv = x,
        xmaxv = x + violinwidth * (xmax - x)
      )

      # Make sure it's sorted properly to draw the outline
      newdata <- rbind(
        plyr::arrange(transform(data, x = xminv), y),
        plyr::arrange(transform(data, x = xmaxv), -y)
      )

      # Close the polygon: set first and last point the same
      # Needed for coord_polar and such
      newdata <- rbind(newdata, newdata[1, ])

      ggplot2:::ggname("geom_flat_violin", GeomPolygon$draw_panel(newdata, panel_scales, coord))
    },

    draw_key = draw_key_polygon,

    default_aes = aes(
      weight = 1, colour = "grey20", fill = "white", size = 0.5,
      alpha = NA, linetype = "solid"
    ),

    required_aes = c("x", "y")
  )
```

We create a raincloud plot for the noun tasks:

```{r noun-cloud}
noun_tasks <- task_by_subj_l %>%
    filter(task %in% (c('np','nc')))
noun_tasks$task <- recode_factor(noun_tasks$task, nc = 'Noun Comprehension', np = 'Noun Production')

x_dodge <- .1
palette <- 'Set1'

raincloud <- ggplot(noun_tasks, aes(x = task, y = correct, fill = cards)) +
  geom_flat_violin(aes(fill = cards), position = position_nudge(x = x_dodge, y = 0), adjust = 1.5, trim = FALSE, alpha = .5, colour = NA) +
  geom_point(aes(x = as.numeric(task)-.15, y = correct, colour = cards), position = position_jitter(width = .05), size = .25, shape = 20) +
  stat_summary(geom="errorbar", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), width = .1, fun.data=mean_cl_boot) +
  stat_summary(geom="point", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), fun.data=mean_cl_boot) +
  stat_summary(geom="line", position = position_nudge(x = x_dodge, y = 0), mapping = aes(group = cards, colour = cards), fun.data=mean_cl_boot, linetype = 3) +
  scale_colour_brewer(palette = palette) +
  scale_fill_brewer(palette = palette)

raincloud

ggsave(filename = "noun_cloud.pdf", plot = raincloud, units = "cm", width = 15, height = 10)
```

```{r predicate-cloud}
predicate_tasks <- task_by_subj_l %>%
    filter(task %in% (c('pc','pp')))
predicate_tasks$task <- recode_factor(predicate_tasks$task, nc = 'Noun Comprehension', np = 'Noun Production')

x_dodge <- .1
palette <- 'Set1'

raincloud <- ggplot(predicate_tasks, aes(x = task, y = correct, fill = cards)) +
  geom_flat_violin(aes(fill = cards), position = position_nudge(x = x_dodge, y = 0), adjust = 1.5, trim = FALSE, alpha = .5, colour = NA) +
  geom_point(aes(x = as.numeric(task)-.15, y = correct, colour = cards), position = position_jitter(width = .05), size = .25, shape = 20) +
  stat_summary(geom="errorbar", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), width = .1, fun.data=mean_cl_boot) +
  stat_summary(geom="point", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), fun.data=mean_cl_boot) +
  stat_summary(geom="line", position = position_nudge(x = x_dodge, y = 0), mapping = aes(group = cards, colour = cards), fun.data=mean_cl_boot, linetype = 3) +
  scale_colour_brewer(palette = palette) +
  scale_fill_brewer(palette = palette)

raincloud

ggsave(filename = "predicate_cloud.pdf", plot = raincloud, units = "cm", width = 15, height = 10)
```

The raincloud plot for the noun tasks indicates that the data was not normally distributed. Given the small sample sizes, a non-parametric test is most suited for analysing this data. Although the predicate data looks normally distributed, in this study, a Mann-Whitney U test was used to compare the scores for the Italian and English cards for both the noun and predicate tasks.

We start by creating some the summary statistics which are used to interpret the Mann-Whitney U test.

```{r cards-rank}
task_by_subj_l %>%
  group_by(task, cards) %>%
  drop_na() %>%
  summarise(n = n(),
            median = median(correct),
            mean_rank = mean(rank(correct)),
            sum_rank = sum(rank(correct)))
```

Next we run the Mann-Whitney U for each group.

```{r mann-whitney}
# independent 2-group Mann-Whitney U Test
mann_whitney <- function(df, group) {
  df <- drop_na(df) # remove excluded subjects
  n1 <- sum(as.integer(df$cards) == 1) # n in first group
  wilcox.test(correct ~ cards, df) %>%
    with(tibble(U = statistic,
              W = statistic + n1 * (n1 + 1) / 2,
              Z = qnorm(p.value / 2),
              p = p.value))
}

wing_mann_whitney <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(mann_whitney)

pander(wing_mann_whitney)
```

All _p_ values were > 0.05, suggesting there were no differences between the Italian and English cards on any of the tests.

# Non-target but semantically related errors

NTS means non-target but semantically related e.g. the card was "house" but they said "hut".

NTS ~ Production(NP/PP) * Card Language(E/I)

* Plot mean semantically related responses (NTS) ~ NNTS/PNTS * E/I with CI bars


* "A Mann-Whitney-U test was run to determine if there were any statistically
significant differences in the number of non-target but semantically related errors
elicited by the English WinG cards and the Italian WinG cards in the noun production
sub-test. ... The number of non-target but semantically related errors was not statistically significantly different between the English cards (Mdn = 3) and the Italian cards (Mdn = 3), U = 65, z = .415, p = .713"

* "A Mann-Whitney-U test was run to determine if there were differences in the
number of non-target but semantically related errors elicited by the English WinG
cards and the Italian WinG cards in the predicate production sub-test. Distributions of
the scores for the English cards and the Italian cards were not similar, as assessed
by visual inspection. The number of correct responses for the English cards (mean
rank = 8.60) and the Italian cards (mean rank = 10.62) were not statistically
significantly different, U = 31, z = -.821, p = .460"

# Differences for individual cards

Relevant worksheets: [Better tables](#better-tables.html).

We would like to know if there were any accuracy differences between the individual Italian and English cards in each task. We can summarise this data as a table of t-tests. Running a t-test where both groups have identical scores produces an error, so we remove these cards from our tests. For noun comprehension, this was the case for the words 'apple', 'cow', 'penguin','hat', and 'motorbike'.

```{r t-nc}
nc_long <- nc %>%
  select(-correct) %>%
  mutate(cards = factor(cards)) %>%
  mutate(cards = recode_factor(.$cards, `1` = 'italian', `2` = 'english')) %>%
  pivot_longer(cols=(mountain:wellyboots),
               names_to = 'word',
               values_to = 'answer')

correct_word <- function(df, group) {
  df %>% select(answer) %>% summarise(correct = colSums(. == 'C' | . == 'C*'))
}
nc_long_correct <- nc_long %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

t_words <- function(df, group) {
  bf <- ttestBF(formula=correct ~ cards, data = data.frame(df))
  extractBF(bf)
}

nc_bf <- nc_long_correct %>%
  filter(! word %in% c('apple', 'cow', 'penguin','hat','motorbike')) %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

nc_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

We generate similar tables for noun production ...

```{r t-np}
np_long <- np %>%
  select(-correct) %>%
  mutate(cards = factor(cards)) %>%
  mutate(cards = recode_factor(.$cards, `1` = 'italian', `2` = 'english')) %>%
  pivot_longer(cols=(beach:gloves),
               names_to = 'word',
               values_to = 'answer')

np_long_correct <- np_long %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

np_bf <- np_long_correct %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

np_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

... predicate comprehension ...

```{r t-pc}
pc_long <- pc %>%
  select(-correct) %>%
  mutate(cards = factor(cards)) %>%
  mutate(cards = recode_factor(.$cards, `1` = 'italian', `2` = 'english')) %>%
  pivot_longer(cols=(big:pulling),
               names_to = 'word',
               values_to = 'answer')

pc_long_correct <- pc_long %>%
  filter(! word %in% c('climbing','waving', 'running', 'drinking', 'walking')) %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

pc_bf <- pc_long_correct %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

pc_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

... and predicate production.

```{r t-pp}
pp_long <- pp %>%
  select(-correct) %>%
  mutate(cards = factor(cards)) %>%
  mutate(cards = recode_factor(.$cards, `1` = 'italian', `2` = 'english')) %>%
  pivot_longer(cols=(small:pushing),
               names_to = 'word',
               values_to = 'answer')

pp_long_correct <- pp_long %>%
    filter(! word %in% c('far-apart', 'heavy', 'in-front')) %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

pp_bf <- pp_long_correct %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

pp_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

The only Bayes factor which provide evidence of a difference between the cards was `bag` on the noun production task.

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 

