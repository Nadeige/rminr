---
title: "Reliability of Targets in a Picture Naming Task"
author: "Allegra Cattani, Adele Conn, Paul Sharpe and Andy Wills"
output:
  html_document:
    highlight: pygment
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Data required to knit
## https://github.com/ajwills72/rminr-data/tree/master/going-further/picture-naming.xlsx
##
## I check out rminr-data and make a symbolic link to going-further

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and output.
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache = TRUE)
library(pander)

# Line numbering guide
# https://blog.atusy.net/submodules/rmd-line-num/index.html
```

## Before you start

This is an advanced worksheet, which assumes you have completed the [Absolute Beginners' Guide to R](https://ajwills72.github.io/rminr/#beginners) course, the [Research Methods in Practice (Quantitative section)](https://benwhalley.github.io/rmip/overview-quantitative.html) course, and the [Intermediate Guide to R](https://ajwills72.github.io/rminr/#rmip) course. 

## Contents

- [Introduction](#intro)
- [Loading data](#data)
- [Preprocessing demographics data](#preproc-demographics)
- [Preprocessing WinG noun comprehension data](#preproc-nc)
- [Applying exclusion criteria](#exclude)
- [Comparing parents' ratings of their child's language ability](#cdi-diffs)
- [Comparing gender on WinG accuracy](#wing-gender)
- [Exploring correlations between CDI and WinG](#correlate-cdi-wing)
- [Plotting WinG accuracy by card set](#plot-task)
- [Comparing WinG accuracy by card set using Mann-Whitney U tests](#mann-whitney)
- [Comparing semantically related production errors by card set](#compare-production)

<a name="intro"></a>

## Introduction

This worksheet describes a full analysis pipeline for an undergraduate student dissertation on childrenâ€™s language development. This study was an experiment which evaluated the _Words in Game_ (WinG) test. WinG consists of a set of picture cards which are used in four tests: noun comprehension, noun production, predicate comprehension, and predicate production. The Italian and English versions of the WinG cards use different pictures to depict the associated words.

An earlier study found a difference between the English and Italian cards, for adults' ratings of how well each picture represented the underlying construct. In this study, the researchers hypothesised that this difference would influence children's WinG task scores, depending on which set of cards they were tested with. The experiment compared WinG performance of English-speaking children, aged approximately 30 months, tested with either the Italian or English cards. Therefore, this was a 4 (WinG task) x 2 (cards) mixed design.

<a name="data"></a>

## Loading data

Open the `rminr-data` project we used [previously](preproc.html#load).

Ensure you have the latest files by asking git to "`pull`" the repository. Select the `Git` tab, which is located in the row of tabs which includes the `Environment` tab. Click the `Pull` button with a downward pointing arrow. A window will open showing the files which have been pulled from the repository. Close the `Git pull` window. The `case-studies` folder should contain a folder named `allegra-cattani`.

Next, create a new, empty R script and save it in the `rminr-data` folder as `wing.R`. Put all the commands from this worksheet into this file, and run them from there. Save your script regularly.

We start by reading the data.

```{r load, message=FALSE}
rm(list = ls()) # clear the environment
library(tidyverse)

# read data
demographics <- read_csv('case-studies/allegra-cattani/demographics.csv')
nc <- read_csv('case-studies/allegra-cattani/noun_comprehension.csv')
np <- read_csv('case-studies/allegra-cattani/noun_production.csv')
pc <- read_csv('case-studies/allegra-cattani/predicate_comprehension.csv')
pp <- read_csv('case-studies/allegra-cattani/predicate_production.csv')
```

**Explanation of commands:**

1. We clear the workspace, and load the `tidyverse`package.
1. Data for each of the WinG tasks is stored in its own CSV file. There is also file containing 'demographics' data. We read each file into its own data frame.

<a name="preproc-demographics"></a>

## Preprocessing demographics data

Next we preprocess the demographics data.

```{r demographics, message=FALSE}
# preprocess demographics data
demographics <- demographics %>%
  set_names(~ str_to_lower(.)) %>%
  mutate(subj = factor(subj), gender = factor(gender)) %>%
  mutate(gender = recode_factor(.$gender, Male = 'male',
                                Female = 'female')) %>%
  select(subj, gender, cdi_u, cdi_s)
```

**Explanation of commands:**

We convert the column names to lower case using `set_names(~ str_to_lower(.))`. We select the `gender`, `cdi_u` and `cdi_s` columns. These latter two columns contain scores for the Communicative Development Inventory (CDI). The CDI is a list of 100 words with columns 'understands' and 'says'. For each word, the child's parent placed a tick in these columns if they thought their child could understand and/or say the word. The scores for 'understands' (`cdi_u`) and 'says' (`cdi_s`) are a total of the ticked boxes in the two columns. We convert `gender` to a factor and recode it's levels to lower case. Finaly we `select()` the columns we want to keep.

Our `demographics` data frame now contains the child's subject number, their gender and their parents' CDI scores:

```{r echo=FALSE}
demographics %>% head(3) %>% pander()
```

<a name="preproc-nc"></a>

## Preprocessing WinG noun comprehension data

Next we preprocess the WinG data.

```{r nc}
# preprocess noun comprehension data
nc <- nc %>%
  set_names(~ str_to_lower(.)) %>%
  mutate(subj = factor(subj), cards = factor(cards)) %>%
  select(subj, cards, mountain:wellyboots)
```

**Explanation of commands:**

We convert the column names to lower case. The `cards` column indicates whether the child was tested with the Italian or English cards, so we convert it to a factor along with `subj`. We select these columns and the columns containing rating data for the words used in the the noun comprehension task. These are in the column range `mountain:wellyboots`.

Here are the first few rows of `nc`:

```{r echo=FALSE}
nc %>% head(3) %>% pander(split.table = Inf)
```
<a name="exclude"></a>

## Applying exclusion criteria

Before we go any further, we need to apply some exclusion criteria to our data. When the experimenters test children with WinG, they use a textual coding scheme to record each child's responses. The codes we're interested in here are:

* `N/A` (not to be confused with the `R` data type `NA`) - researchers used this code to indicate that the task was interrupted for some reason (e.g. the child began crying).
* `C` or `C*` - `C` indicates that the child responded correctly for the picture on the card, `C*` indicates that the response was a correct synonym. In this experiment, both of these values are considered correct responses.
* `NTS` - This stands for "non-target but semantically related". This code is used in the noun and predicate production tests, for example if the picture on the card was a house but the child said "hut".

In this experiment, the exclusion criteria were as follows:

1. Any subject with 'N/A' for one of the first 17 words in a task. This criterion excludes children with insufficient data for analysis.
1. Any subject in the remaining data whose accuracy was less than two standard deviations below the mean. This is a more general criterion to exlude subjects who had especially low scores for various reasons.

### Defining a function

Although these exclusion criteria are quite easy to describe, writing the code to apply them is a bit more complex. Furthermore, we need to apply these criteria separately for each of the four WinG tasks. Writing the same code four times would make our program harder to read and is error prone. If we were to discover that we'd made a mistake, we would have to fix it in four different places. Fortunately, we can just write the code once and then reuse it. We do this using a function.

Functions are reusable lines of code which perform a particular function (hence the name), and are one of the most powerful features of programming languages such as R. At this point you will be familiar with _using_ functions. For example `rm()` is a function for removing objects from the R environment, and is part of the base `R` language. The `rm()` function is reusable in the sense that you can use the same function to do slightly different things. For example, `rm('foo')` would remove the single object `foo` from your environment. The string `foo` is called an _argument_ and is the data processed by `rm()`. The same function can also remove multiple variables provided in a list, so calling `rm(ls())`, first calls `ls()` which lists all objects in your environment, and passes the results as an argument to `rm()` which removes them, thereby cleaning your environment of all objects. Similarly, you've used `read_csv()`, with different filename arguments to read different data files. The `read_csv()` function is part of the `readr` package which is loaded when you call `library(tidyverse)` (`library()` is another base `R` function). 

Removing objects from the environment is such a common task that it's a function included with `R`. Reading files is almost as common, which is why it is part of the `readr` package that you load with `library(tidyverse)`. However, `R` also includes a special function called `function()`, which you can use to define your own functions. This is not particularly complicated, but requires a little explanation if you've never encountered the idea before (you can find more details in [the R manual](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Writing-your-own-functions)). We'll explain the steps by writing a function called `exclude()`, allowing us to apply our exclusion criteria for each of the four WinG tests.

```{r exclude}
# apply WinG exclusion criteria
exclude <- function(df) {
  # exclude if N/A in item 17 or lower
  logical_matrix <- df == 'N/A'
  q17 <- logical_matrix %>%
    which(arr.ind = TRUE) %>%
    data.frame() %>%
    group_by(row) %>%
    summarise(min = min(col)) %>%
    mutate(subj = factor(row)) %>%
    select(subj, min)
  q17 <- left_join(df, q17, by='subj') %>%
    replace_na(list(min = 20))
  q17 <- q17 %>% filter(min > 17) %>% select(-min)
  
  # calculate total correct and semantically related
  q17 <- q17 %>% mutate(correct = rowSums(. == 'C' | . == 'C*'),
                        related = rowSums(. == 'NTS'))
  
  # exclude participants with scores < 2 sd below the mean
  q17 <- q17 %>% filter(correct < mean(correct) + 2 * sd(correct))

  return(q17)
}
```

**Explanation of commands:**

1. Our `exclude()` function accepts a data frame argument `df`, containing the data for a single WinG task.
1. `logical_matrix <- df == 'N/A'` converts `df` to a matrix containing the value `TRUE` where a cell in `df` contained the string `N/A`, or `FALSE` otherwise.
1. `which(arr.ind = TRUE)` creates a matrix with columns `row` (this will be the same as our subject number) and `col` containing the row and column number of any cells with the value `TRUE` (the cells that originally contained `N/A`).
1. We convert this to a data frame which we group by `row`.
1. `summarise(min = min(col))` summarises the grouped data by creating a single row containing the lowest column number that contained `N/A`.
1. `mutate(subj = factor(row)) %>% select(subj, min)` creates a factor `subj` from `row`, and selects this along with the column number of the first column that contained `N/A`. We have now converted our original data into a data from with rows for any subjects who had at least one `N/A` and the number of the first word where the task was interrupted.
1. `q17 <- left_join(df, q17, by='subj')` joins this data frame to the data frame we passed to `exclude()`. For rows where there is no matching `subj`, `min` gets the value `NA`. `replace_na(list(min = 20))` converts these `NA`s to the value `20`, indicating the child provided an answer for all words.
1. To complete this exclusion criterion, `q17 <- q17 %>% filter(min > 17) %>% select(-min)` selects only children who provided answers beyond word 17, and then removes the `min` column, as it's no longer needed.
1. Next we calculate accuracy scores for each subject across all words. `q17 <- q17 %>% mutate(correct = rowSums(. == 'C' | . == 'C*'))` creates a new column `correct` with a value for each row which is the sum of the number of columns containing `C` or `C*` (`|` means 'or'). Cells containing `C` indicate that the child responded correctly for the picture on the card for the the word in this column. Cells containing `C*` indicates that the response was a correct synonym. In this experiment, both of these values are considered correct responses. Similarly, we calculate the total number columns containing `NTS` and store this in the column `related`. We'll use this value in a later analysis.
1. Now we apply the second exclusion criterion. `q17 %>% filter(correct < mean(correct) + 2 * sd(correct))` calculates the mean and standard deviation for `correct`, and then filters any subjects whose value for `correct` was less than two standard deviations below the mean.
1. Finally, we return the data frame without rows meeting our two exclusion critera.

Now we can use `exclude()` to apply the exclusion criteria to our data.

```{r exclude-nc}
nc_by_subj <- nc %>% exclude() %>% select(subj, correct, related)
```

**Explanation of commands:**

We pipe `nc` into `exclude()`, which applies our exclusion criteria. As a side-effect, it also calculates the `correct` and `related` scores for each subject.

Looking at `nc_by_subj`, we can see that subject `18` was excluded from the noun comprehension task. The `related` scores are all 0 because this column is only relevant for the word production tasks.

```{r, echo=FALSE}
nc_by_subj %>% pander()
```

We repeat these steps to apply exclusion critera for the noun production, predicate comprehension and predicate production tasks.

```{r np-pc-pp}
np <- np %>%
  set_names(~ str_to_lower(.)) %>%
  mutate(subj = factor(subj), cards = factor(cards)) %>%
  select(subj, cards, beach:gloves)
np_by_subj <- np %>% exclude() %>% select(subj, correct, related)

pc <- pc %>%
  set_names(~ str_to_lower(.)) %>%
  mutate(subj = factor(subj), cards = factor(cards)) %>%
  select(subj, cards, big:pulling)
pc_by_subj <- pc %>% exclude() %>% select(subj, correct, related)

pp <- pp %>%
  set_names(~ str_to_lower(.)) %>%
  mutate(subj = factor(subj), cards = factor(cards)) %>%
  select(subj, cards, small:pushing)
pp_by_subj <- pp %>% exclude() %>% select(subj, correct, related)
```
**Explanation of commands:**

This code is the same as for the noun comprehension task, apart the range of columns we `select()` for each task. For noun production, the scores are in `beach:gloves`, for predication comprehension they're in `big:pulling`, and for predication production they're in `small:pushing`.

Having preprocessed the demographics data and applied our exclusions to the WinG data, we now join these data frames together in preparation for calculating our descriptive statistics.

```{r join}
# join data
task_by_subj <- nc %>% select(subj, cards) %>%
  full_join(., demographics, by='subj')
```

**Explanation of commands:**

We start by creating a data frame containing `subj` and `cards` for all participants, and joining it with the `demographics` data frame. We use `nc` to create the cards data, as this was a data frame _before_ we applied any exclusion critera. If we didn't do this, in the next stage we would end up with `NA` values in `cards` for excluded participants.

Here's `task_by_subj`:

```{r echo=FALSE}
task_by_subj %>% pander()
```

We join `task_by_subj` to the other WinG task data frames:

```{r join-more}
task_by_subj <- task_by_subj %>%
  full_join(., nc_by_subj, by='subj') %>%
  full_join(., np_by_subj, by='subj', suffix = c('_nc','_np')) %>%
  full_join(., pc_by_subj, by='subj') %>%
  full_join(., pp_by_subj, by='subj', suffix = c('_pc', '_pp')) %>%
  mutate(nc = correct_nc, np = correct_np, pc = correct_pc, pp = correct_pp) %>%
  select(subj, gender, cards, nc, np, pc, pp, cdi_u, cdi_s,
         related_nc, related_np, related_pc, related_pp)
```

**Explanation of commands:**

Each `full_join()`, joins an additional data frame by matching values in `subj`. The `suffix = c('_nc','_np')` argument adds suffixes to disambiguate columns with the same name, in this case `correct` and `related`. We use `mutate()` with `select()` to remove the `correct_` prefix from the 'correct' columns. We leave the `related_` prefix on the 'syntactically related' columns.

Our data is now fully preprocessed. Notice how the join sets cells to the value `NA` for subjects who were excluded for that particular task.

```{r echo=FALSE}
task_by_subj %>% pander(split.table = Inf)
```

<a name="cdi-diffs"></a>

## Comparing parents' ratings of their child's language ability

For our first analysis, we want to check that there were no differences in language ability between the children assigned to the Italian and English card groups. We do this using between-subjects t-tests of their parents' CDI ratings. Bayesian t-tests were introduced in the [Evidence worksheet](evidence.html).

```{r cdi}
# compare children's language ability using CDI
library(BayesFactor, quietly=TRUE)
cdi_u_bf <- ttestBF(formula=cdi_u ~ cards, data = task_by_subj)
cdi_u_bf
cdi_s_bf <- ttestBF(formula=cdi_s ~ cards, data = task_by_subj)
cdi_s_bf
```

**Explanation of commands:**

First we load the `BayesFactor` package. Next, we run a t-test which compares CDI 'understands' (`cdi_u`) for the two card sets. We another t-test which compares CDI 'says' (`cdi_s`) for the two card sets.

**Explanation of output:**

Here, we're hoping to find evidence for the null hypothesis i.e. no differences in the means for the two groups. Our Bayes factors (CDI understands = `r round(BayesFactor::extractBF(cdi_u_bf)$bf, 2)`; CDI says = `r round(BayesFactor::extractBF(cdi_s_bf)$bf, 2)`) indicate that it's 2-2.5 times more likely that there's no difference, than there is a difference in language ability between the children tested using the Italian and English cards.

<a name="wing-gender"></a>

## Comparing gender on WinG accuracy

We would also like to check that there were no gender differences on any of the tests. We'll present this analysis in a table containing descriptive statistics and t-tests. This is described in more detail in the [Better tables worksheet](#better-tables.html).

We start by calculating means and standard deviations for each WinG task by gender.

```{r descriptives}
# table of descriptives and t-tests for WinG by gender
task_by_subj_l <- task_by_subj %>%
  pivot_longer(cols = c(nc, np, pc, pp),
               names_to = 'task',
               values_to = 'correct')

descriptives <- task_by_subj_l %>%
  group_by(task, gender) %>%
  summarise(mean = mean(correct, na.rm = TRUE),
            sd = sd(correct, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate_if(is.numeric, round, 2)

descriptives <- descriptives %>%
  pivot_wider(names_from = gender, values_from = c(mean, sd)) %>%
  select(task, mean_male, sd_male, mean_female, sd_female)
```

**Explanation of commands:**

We convert `task_by_subj` to long format by using `pivot_longer()` on the correct score columns `nc`, `np`, `pc` and `pp`. We use this to calculate a mean and standard deviation by gender within task. When calculating these summary statistics, we need the argument `na.rm = TRUE` to ignore missing data for participants who were excluded from any of the tests. We round the results to 2 decimal places. We use `pivot_wider()` so that `descriptives` has a row for each task, and columns containing the mean and standard deviation by gender.

Our table of descriptive statistics now looks like this:

```{r echo=FALSE}
descriptives %>% pander()
```

Now we run our Bayesian t-tests to test for gender differences on each of the tasks, and add the results to our table.

```{r WinG-t-by-gender}
# t-test for accuracy by gender
t_gender <- function(df, group) {
  df <- drop_na(df) # remove data for excluded subjects
  bf <- ttestBF(formula=correct ~ gender, data = data.frame(df))
  extractBF(bf)
}

gender_bf <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(t_gender) %>%
  mutate(bf = round(bf,2)) %>%
  select(task, bf)

descriptives_t <- full_join(descriptives, gender_bf, by = 'task')
```

**Explanation of commands:**

We define the function `t_gender()` to run a between-subjects t-test comparing task accuracy (`correct`) by `gender`. Before running the test, we use `drop_na(df)` to remove rows for participants who were excluded for the task. The function returns the Bayes factor. We group our long format task data by task, and use `group_modify(t_gender)` to calculate the Bayes factor for the grouped data. We round the Bayes factors to 2 decimal places and then select the `task` and `bf` columns. We join this to our `descriptives` data frame.

Our data frame now looks like this:

```{r echo=FALSE}
descriptives_t %>% pander()
```

We tidy up the data and then present the data frame as a table.

```{r descriptives-presentation}
descriptives_t <- descriptives_t %>%
  unite("male", mean_male:sd_male, sep=' (') %>%
  unite("female", mean_female:sd_female, sep=' (') %>%
  mutate(male = paste0(male, ')'), female = paste0(female, ')'), task = factor(task),
         task = recode_factor(.$task, nc = 'Noun Comprehension', np = 'Noun Production',
                              pc = 'Predicate Comprehension', pp = 'Predicate Production'))

library(kableExtra)
descriptives_t %>% kable(
  col.names = c('Task', 'Male', 'Female', 'Bayes Factor')) %>%
  kable_styling()
```

**Explanation of commands:**

We use `unite()` to combine the male and female summary statistics into a single column with the standard deviation in parentheses. We rename `task` factor levels so that the reader will understand what they mean. Finally, we print the table as HTML, a format we could copy-paste into a word processor

**Explanation of output:**

The Bayes factors mean that it's about twice as likely that there are no gender differences than that there are, for noun comprehension and predicate production. For noun production, it's about 1.5 times as like that there isn't a gender difference, than that there is. For predicate comprehension, the evidence for or against a gender difference is about equal.

This table could be included in a reporty by copy-pasting into a word processor, and then styled according to APA guidelines.

<a name="correlate-cdi-wing"></a>

## Exploring correlations between CDI and WinG

We would also like to know if parents' ratings of their child's ability using the CDI was related to their child's performance on the WinG tasks. We start by looking at correlations between CDI 'understands' comprehension scores on the noun and predicate tests.

```{r cdi_u_correlations}
# correlations between CDI and WinG
cdi_u_nc_cor <- cor(method = 'spearman', task_by_subj$cdi_u, task_by_subj$nc, use="complete.obs")
cdi_u_nc_cor
cdi_u_nc_bf <- correlationBF(task_by_subj$cdi_u, task_by_subj$nc)
cdi_u_nc_bf

cdi_u_pc_cor <- cor(method = 'spearman', task_by_subj$cdi_u, task_by_subj$pc, use="complete.obs")
cdi_u_pc_cor
cdi_u_pc_bf <- correlationBF(task_by_subj$cdi_u, task_by_subj$pc)
cdi_u_pc_bf
```

**Explanation of commands:**

We use a Spearman correlation to test the relationship between `task_by_subj$cdi_u` and `task_by_subj$nc` (noun comprehension). This was introduced in the [More on relationships, part 2 worksheet](corr-extended.html). The option `use="complete.obs"` deletes any cases with a value of `NA`. (Remember that we assigned the value `NA` to cells for subjects who were excluded from a test.) We also calculate a Bayes Factor to test the reliability of the relationship. We use similar commands to test the relationship between `cdi_u` and predicate comprehension (`pc`).

**Explanation of output:**

There doesn't appear to be a relationship between parents' CDI 'understands' scores and children's noun comprehension (_rs_ = `r round(cdi_u_nc_cor,2)`), although there is weak evidence against this conclusion (_BF_ = `r round(BayesFactor::extractBF(cdi_u_nc_bf)$bf,2)`). There seems to be a weak negative correlation between parents' CDI 'understands' scores and children's predicate comprehension (_rs_ = `r round(cdi_u_pc_cor,2)`), however, the Bayes factor of `r round(BayesFactor::extractBF(cdi_u_pc_bf)$bf,2)` means it's almost twice as likely that there isn't a relationship between these two variables, than that there is.

Now we look at correlations between parents' ratings of their child's ability to say words (CDI says), and the childrens' noun and predicate production scores.

```{r cdi_s_correlations}
cdi_s_np_cor <- cor(method = 'spearman', task_by_subj$cdi_s, task_by_subj$np, use="complete.obs")
cdi_s_np_cor
cdi_s_np_bf <- correlationBF(task_by_subj$cdi_s, task_by_subj$np)
cdi_s_np_bf

cdi_s_pp_cor <- cor(method = 'spearman', task_by_subj$cdi_s, task_by_subj$pp, use="complete.obs")
cdi_s_pp_cor
cdi_s_pp_bf <- correlationBF(task_by_subj$cdi_s, task_by_subj$pp)
cdi_s_pp_bf
```

**Explanation of commands:**

These calculations are identical to those above, except we are looking at the evidence for relationships between `cdi_u` and `task_by_subj$np` (noun production), and `task_by_subj$pp` (predicate production).

**Explanation of output:**

There is evidence of a strong, positive correlation between parents' CDI 'says' scores and children's noun production (_rs_ = `r round(cdi_s_np_cor,2)`, _BF_ = `r round(BayesFactor::extractBF(cdi_s_np_bf)$bf,2)`). There doesn't appear to be a relationship between CDI 'says' and children's predicate production (_rs_ = `r round(cdi_s_pp_cor,2)`, _BF_ = `r round(BayesFactor::extractBF(cdi_s_pp_bf)$bf,2)`).

In summary, noun production was the only task in which parents' ratings of their childrens' ability matched the childrens' accuracy.

<a name="plot-task"></a>

## Plotting WinG accuracy by card set

We're now ready to test our main hypothesis, which predicts that there will be a difference WinG task scores, depending on which set of cards the children were tested with. We'll start by creating plots to show the distribution of scores for the two card sets on the WinG tasks.

```{r noun-cloud, class.source = 'numberLines lineAnchors'}
task_by_subj_l$task <- recode_factor(task_by_subj_l$task, nc = 'Noun Comprehension',
                                     np = 'Noun Production', pc = 'Predicate Comprehension',
                                     pp = 'Predicate Production')

library(see)
ggplot(task_by_subj_l, aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.3, size=0) +
  scale_fill_material_d() + xlab('WinG Task') + ylab('Accuracy (max = 20)')
```

```{r, eval=FALSE, echo=FALSE}
noun_tasks <- task_by_subj_l %>%
    filter(task %in% (c('nc')))
noun_tasks$task <- recode_factor(noun_tasks$task, nc = 'Noun Comprehension')

ggplot(noun_tasks, aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.3, size=0)

half_violin <- ggplot(task_by_subj_l, aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.3, size=0) +
#  stat_summary(geom="point", mapping = aes(colour = cards), fun.data=mean_cl_boot) +
  scale_fill_material_d()
half_violin + facet_wrap(~ task, ncol=2)
```

**Explanation of commands:**

**Explanation of output:**

We plotted the data in this format to get an indication of whether there were differences between the Italian and English cards on any of the tests. This looks unlikely, given the extensive overlap in scores between the card sets. The plot also shows that the data were slightly skewed on the noun tasks due to some low scores.

<a name="mann-whitney"></a>

## Comparing WinG accuracy by card set using Mann-Whitney U tests

Although the plot suggests that the predicate data was normally distributed, given the small sample sizes, a non-parametric was considered most suited for analysing this data. We'll use Mann-Whitney U tests to compare each task score for the Italian and English cards. The Mann-Whitney U test is explained in more detail in the [Traditional non-parametric tests worksheet](non-parametric.html).

We start by creating some summary statistics which are used to interpret this test.

```{r cards-rank}
task_by_subj_l %>%
  group_by(task, cards) %>%
  drop_na() %>%
  summarise(n = n(),
            median = median(correct),
            mean_rank = mean(rank(correct)),
            sum_rank = sum(rank(correct)))
```

**Explanation of commands:**

1. We group our data by the two card sets, within the four tests.
1. `drop_na()` removes any subjects where were excluded from a test.
1. We use `summarise`, to calculate the number of cases in each group `n()`, and the `median()` score. The `rank()` function orders the scores (lowest to highest), and assigns each a score a number according to its ordered position. We use `mean(rank(correct)` to calculate the "mean rank" score and `sum(rank(correct)` to calculate the "sum rank" score.

**Explanation of output:**

As the plots suggested, the median, mean rank, and sum rank scores are very similar for the Italian and English cards in each task.

We run Mann-Whitney tests to make a direct comparison between the Italian and English cards for each WinG task.

```{r mann-whitney}
# independent 2-group Mann-Whitney U Test
mann_whitney <- function(df, group) {
  df <- drop_na(df) # remove excluded subjects
  n1 <- sum(as.integer(df$cards) == 1) # n in first group
  wilcox.test(correct ~ cards, df) %>%
    with(tibble(U = statistic,
              W = statistic + n1 * (n1 + 1) / 2,
              Z = qnorm(p.value / 2),
              p = p.value))
}

wing_mann_whitney <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(mann_whitney)
wing_mann_whitney
```

**Explanation of commands:**

The `mann_whitney()` function is explained in detail in the [Traditional non-parametric tests worksheet](non-parametric.html). In summary, it uses `wilcox.test()`, with the argument `paired = FALSE` (the default) which the equivalent to a Mann-Whitney U test. The results are adjusted so that the function reports the same values as `SPSS`. The `W` statistic generated by `R` is the same as `U` in SPSS. The `p` value in `R` is the same as `SPSS`. The `W` and `Z` statistics reported by `SPSS` are not calculated by `wilcox.test()`, so we calculate these manually. The warning `cannot compute exact p-value with ties` lets you know that the method used to calculate _p_ will not be exact, because some items in the Italian and English scores had identical rankings.

**Explanation of output:**

As this is a traditional statistical test, the p-value indicates whether there was a significant difference between the cards on any of the tasks. All _p_ values were > 0.05, suggesting there were no differences.

<a name="compare-production"></a>

## Comparing semantically related production errors by card set

We would also like to know if there were any differences for the semantically related errors between the two sets of cards.

Again, we start with a plot.

```{r nts-production-plot}
semantically_related <- task_by_subj %>%
  select(subj, cards, related_np, related_pp) %>%
  pivot_longer(cols = c(related_np, related_pp),
               names_to = 'task',
               values_to = 'correct') %>%
  mutate(task = recode_factor(.$task, related_np = 'Noun Production',
                              related_pp = 'Predicate Production'))

ggplot(semantically_related, aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.3, size=0) +
  scale_fill_material_d() + xlab('WinG Task') +
  ylab('Semantically related errors (max = 20)')
```

**Explanation of commands:**

**Explanation of output:**

The extensive overlap in the distribution of scores suggests there were no differences between the Italian and English cards for semantically related errors on the production tasks.

We run some Mann-Whitney U tests to test this.

```{r nts-production-mw}
production_mann_whitney <- semantically_related %>%
  group_by(task) %>%
  group_modify(mann_whitney)
production_mann_whitney
```

**Explanation of commands:**

We group `semantically_related` by task, and reuse our `mann_whitney()` function to run a Mann-Whitney test for the noun and predicate production tasks.

**Explanation of output:**

The _p_ values for both tests are > 0.05, indicating that there was no difference between the card sets for semantically related errors on the production tasks.



If we do not find differences between the two sets of cards in either study it will
demonstrate the reliability of the targets used in the English WinG cards because it
will show that despite the changes in the images on the cards they still elicit the same
responses as the Italian WinG cards.
___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 

