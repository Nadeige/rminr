---
title: "Reliability of Targets in a Picture Naming Task"
author: "Allegra Cattani, Adele Conn, Paul Sharpe and Andy Wills"
output: html_document
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate

## Data required to knit
## https://github.com/ajwills72/rminr-data/tree/master/going-further/picture-naming.xlsx
##
## I check out rminr-data and make a symbolic link to going-further

## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)

## Show commands and output.
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache = TRUE)
library(pander)
```

## Before you start

This is an advanced worksheet, which assumes you have completed the [Absolute Beginners' Guide to R](https://ajwills72.github.io/rminr/#beginners) course, the [Research Methods in Practice (Quantitative section)](https://benwhalley.github.io/rmip/overview-quantitative.html) course, and the [Intermediate Guide to R](https://ajwills72.github.io/rminr/#rmip) course. 

## Contents

- [Introduction](#intro)
- [Reading data from Excel](#excel)
- [Preprocessing demographics data](#preproc-demographics)
- [Preprocessing WinG noun comprehension data](#preproc-nc)
- [Applying exclusion criteria](#exclude)

- [Comparing parents' ratings of their child's language ability](#cdi-diffs)
- [Generating a table of statistics](#table-descriptives)
- [Exploring correlations between CDI and WinG](#correlate-cdi-wing)
- [Plotting WinG task scores (raincloud plot)](#plot-task)
- [Comparing cards using Mann-Whitney U tests](#mann-whitney)
- [Comparing semantically related production errors](#compare-production)
- [Comparing individual cards](#table-cards)

<a name="intro"></a>

## Introduction

This worksheet describes a full analysis pipeline for an undergraduate student dissertation on childrenâ€™s language development. This study was an experiment which evaluated the _Words in Game_ (WinG) test. WinG consists of a set of picture cards which are used in four tests: noun comprehension, noun production, predicate comprehension, and predicate production. The Italian and English versions of the WinG cards use different pictures to depict the associated words. The experiment tested whether English-speaking children aged approximately 30 months, produce similar responses for the two sets of cards. This was a 4 (test) x 2 (cards) mixed design.

TODO: Explain rationale and hypotheses.

<a name="excel"></a>

## Reading data from Excel

Much of the pipeline for this experiment is described in more detail in the [Better tables worksheet](#better-tables.html). To avoid repetition, we'll summarise those steps here and only go into detail when discussing code which isn't described in that worksheet.

Open the `rminr-data` project we used [previously](preproc.html#load).

Ensure you have the latest files by asking git to "`pull`" the repository. Select the `Git` tab, which is located in the row of tabs which includes the `Environment` tab. Click the `Pull` button with a downward pointing arrow. A window will open showing the files which have been pulled from the repository. Close the `Git pull` window. The `going-further` folder should contain the file `picture-naming.xlsx`.

Next, create a new, empty R script and save it in the `rminr-data` folder as `wing.R`. Put all the commands from this worksheet into `wing.R`, and run them from there. Save your script regularly.

The data for each WinG test was recorded on a subsheet of an Excel spreadsheet. There was also a subsheet containing demographic data. We start by reading all subsheets into a single data frame.

```{r excel, message=FALSE}
rm(list = ls()) # clear the environment
library(tidyverse)

# read data
library(readxl)
path <- 'going-further/picture-naming.xlsx'
data <- path %>%
  excel_sheets() %>%
  set_names() %>%
  map_df(~ read_excel(path = path, sheet = .x, range = "A1:V20"), .id = "sheet")
```

**Explanation of commands:**

We clear the workspace, and load the `tidyverse` and `readxl` packages. `excel_sheets()` extracts the names of the subsheets from the main Excel spreadsheet. The `map_df()` command reads the cells in the range `A1:V20` from each subsheet and combines them into a single data frame (`data`), with a column `sheet` containing the name of the subsheet.

Here's a selection of rows from `data`:

```{r echo=FALSE}
data %>% slice(1:2, 20:21, 39:40, 58:59, 77:78) %>% pander(split.table = Inf)
```

<a name="preproc-demographics"></a>

## Preprocessing demographics data

First we extract the demographics data into its own data frame.

```{r demographics, message=FALSE}
# demographics data
demographics <- data %>%
  filter(sheet == 'Demographic') %>%
  set_names(~ str_to_lower(.)) %>%
  mutate(gender = factor(gender), subj = factor(seq.int(nrow(.)))) %>%
  mutate(gender = recode_factor(.$gender, Male = 'male',
                                Female = 'female')) %>%
  select(subj, gender, cdi_u, cdi_s)
```

**Explanation of commands:**

We `filter()` the data to select just the demographics rows, and convert the column names to lower case using `set_names(~ str_to_lower(.))`. We select the `gender`, `cdi_u` and `cdi_s` columns. These latter two columns contain scores for the Communicative Development Inventory (CDI). The CDI is a list of 100 words with columns 'understands' and 'says'. For each word, the child's parent placed a tick in these columns if they thought their child could understand and/or say the word. The scores for 'understands' (`cdi_u`) and 'says' (`cdi_s`) are a total of the ticked boxes in the two columns. We add a subject number `subj`, convert `gender` to a factor and recode it's levels to lower case. Finaly we `select()` the columns we want to keep.

Our `demographics` data frame now contains the child's subject number, their gender and their parents' CDI scores:

```{r echo=FALSE}
demographics %>% head(3) %>% pander()
```

<a name="preproc-nc"></a>

## Preprocessing WinG noun comprehension data

Next we preprocess the WinG data.

```{r nc}
# preprocess noun task data
nc <- data %>%
  filter(sheet == 'Noun Comprehension') %>%
  set_names(~ str_to_lower(.)) %>%
  mutate(cards = factor(cards), subj = factor(seq.int(nrow(.)))) %>%
  select(subj, cards, mountain:wellyboots)
```

**Explanation of commands:**

We `filter()` the data to select just the noun comprehension rows and convert column names to lower case. The `cards` column indicates whether the child was tested with the Italian or English cards, so we convert it to a factor. We also generate a sequential subject number `subj`. We select these columns and the columns containing rating data for the words used in the the noun comprehension task. These are in the column range `mountain:wellyboots`.

Here are the first few rows of `nc`:

```{r echo=FALSE}
nc %>% head(3) %>% pander(split.table = Inf)
```
<a name="exclude"></a>

## Applying exclusion criteria

Before we go any further, we need to apply some exclusion criteria to our data. When the experimenters administer WinG, they use a textual coding scheme to record each child's responses. The codes we're interested in here are:

* `N/A` (not to be confused with the `R` data type `NA`) - researchers used this code to indicate that there was some kind of interruption to the task (e.g. the child began crying).
* `C` or `C*` - `C` indicates that the child responded correctly for the picture on the card, `C*` indicates that the response was a correct synonym. In this experiment, both of these values are considered correct responses.
* `NTS` - This stands for "non-target but semantically related". This code is used in the noun and predicate production tests, for example if the card was a house but the child said "hut".

In this experiment, the exclusion criteria were as follows:

1. Any subject with 'N/A' for one of the first 17 words in a task. This criterion excludes children with insufficient data for analysis.
1. Any subject in the remaining data whose accuracy was less than two standard deviations below the mean. This is a more general criterion to exlude subjects who had especially low scores for various reasons.

We'll write a function `exclude()`, which will allow us to apply these criteria for each of the four WinG tests.

```{r exclude}
# apply exclusion criteria to subtest data
exclude <- function(df) {
  # exclude if N/A in item 17 or lower
  logical_matrix <- df == 'N/A'
  q17 <- logical_matrix %>%
    which(arr.ind = TRUE) %>%
    data.frame() %>%
    group_by(row) %>%
    summarise(min = min(col)) %>%
    mutate(subj = factor(row)) %>%
    select(subj, min)
  q17 <- left_join(df, q17, by='subj') %>%
    replace_na(list(min = 20))
  q17 <- q17 %>% filter(min > 17) %>% select(-min)
  
  # calculate total correct and semantically related
  q17 <- q17 %>% mutate(correct = rowSums(. == 'C' | . == 'C*'),
                        related = rowSums(. == 'NTS'))
  
  # exclude participants with scores < 2 sd below the mean
  q17 <- q17 %>% filter(correct < mean(correct) + 2 * sd(correct))

  return(q17)
}
```

**Explanation of commands:**

1. Our `exclude()` function accepts a data frame argument `df`, containing the data recorded for a sub-test.
1. `logical_matrix <- df == 'N/A'` converts `df` to a matrix containgin the value `TRUE` where a cell in `df` contained the string `N/A`, or `FALSE` otherwise.
1. `which(arr.ind = TRUE)` creates a matrix with columns `row` (this will be the same as our subject number) and `col` containing the row and column number of any cells with the value `TRUE` (i.e. those cells that contained `N/A`).
1. Whe convert this to a data frame which we group by `row`.
1. `summarise(min = min(col))` summarises the grouped data by creating a single row containing the lowest column number that contained `N/A`.
1. `mutate(subj = factor(row)) %>% select(subj, min)` creates a factor `subj` from `row`, and selects this along with the column number of the first column that contained `N/A`. We have now converted our original data into a data from with rows for any subjects who had at least one `N/A` and the number of the first word where the task was interrupted.
1. `q17 <- left_join(df, q17, by='subj')` joins this data frame to the data frame we passed to `exclude()`. For rows where there is no matching `subj`, `min` gets the value `NA`. `replace_na(list(min = 20))` converts these `NA`s to the value `20`, indicating the child provided an answer for all words.
1. To complete this exclusion criterion, `q17 <- q17 %>% filter(min > 17) %>% select(-min)` selects only children who provided answers beyond word 17, and then removes the `min` column.
1. Next we calculate accuracy scores for each subject across all words. `q17 <- q17 %>% mutate(correct = rowSums(. == 'C' | . == 'C*'))` creates a new column `correct` with a value for each row which is the sum of the number of columns containing `C` or `C*` (`|` means 'or'). Cells containing `C` indicate that the child responded correctly for the picture on the card for the the word in this column. Cells containing `C*` indicates that the response was a correct synonym. In this experiment, both of these values are considered correct responses. Similarly, we calculate the total number columns containing `NTS` and store this in the column `related`, as it will be useful later.
1. Now we apply the second exclusion criterion. `q17 %>% filter(correct < mean(correct) + 2 * sd(correct))` calculates the mean and standard deviation for `correct`, and then filters any subjects whose value for `correct` was less than two standard deviations below the mean.
1. Finally, we return the data frame without rows meeting our two exclusion critera.

Now we can use `exclude()` to apply the exclusion criteria to our data.

```{r exclude-nc}
nc_by_subj <- nc %>% exclude() %>% select(subj, correct, related)
```

**Explanation of commands:**

We pipe `nc` into `exclude()`, which applies our exclusion criteria. As a side-effect, it also calculates the accuracy scores for each subject. Looking at `nc_by_subj`, we can see that subject `18` was excluded from the noun comprehension task:

```{r, echo=FALSE}
nc_by_subj %>% pander()
```

We repeat these steps to apply exclusion critera for the noun production, predicate comprehension and predicate production tasks.

```{r np-pc-pp}
np <- data %>%
  filter(sheet == 'Noun Production') %>%
  set_names(~ str_to_lower(.)) %>%
  select(cards, beach:gloves) %>%
  mutate(subj = factor(seq.int(nrow(.))))
np_by_subj <- np %>% exclude() %>% select(subj, correct, related)

pc <- data %>%
  filter(sheet == 'Predicate Comprehension') %>%
  set_names(~ str_to_lower(.)) %>%
  select(cards, big:pulling) %>%
  mutate(subj = factor(seq.int(nrow(.))))
pc_by_subj <- pc %>% exclude() %>% select(subj, correct, related)

pp <- data %>%
  filter(sheet == 'Predicate Production') %>%
  set_names(~ str_to_lower(.)) %>%
  select(cards, small:pushing) %>%
  mutate(subj = factor(seq.int(nrow(.))))
pp_by_subj <- pp %>% exclude() %>% select(subj, correct, related)
```
**Explanation of commands:**

This code is the same as for the noun comprehension task, apart from the data we select in each of the `filter()` commands, and the range of columns we `select()` for each task. For noun production, the scores are in `beach:gloves`, for predication comprehension they're in `big:pulling`, and for predication production they're in `small:pushing`.

Having preprocessed the demographics data and applied our exclusions to the WinG data, we now join these data frames together in preparation for calculating our descriptive statistics.

```{r join}
# join data
task_by_subj <- nc %>% select(subj, cards) %>%
  full_join(., demographics, by='subj')
```

**Explanation of commands:**

We start by creating a data frame containing `subj` and `cards` for all participants, and joining it with the `demographics` data frame. We use `nc` to create the cards data, as this was a data frame _before_ we applied any exclusion critera. If we didn't do this, in the next stage we would end up with `NA` values in `cards` for excluded participants.

Here's `task_by_subj`:

```{r echo=FALSE}
task_by_subj %>% pander()
```

We join `task_by_subj` to the other WinG task data frames:

```{r join-more}
task_by_subj <- task_by_subj %>%
  full_join(., nc_by_subj, by='subj') %>%
  full_join(., np_by_subj, by='subj', suffix = c('_nc','_np')) %>%
  full_join(., pc_by_subj, by='subj') %>%
  full_join(., pp_by_subj, by='subj', suffix = c('_pc', '_pp')) %>%
  mutate(nc = correct_nc, np = correct_np, pc = correct_pc, pp = correct_pp) %>%
  select(subj, gender, cards, nc, np, pc, pp, cdi_u, cdi_s,
         related_nc, related_np, related_pc, related_pp)
```

**Explanation of commands:**

Each `full_join()`, joins an additional data frame by matching values in `subj`. The `suffix = c('_nc','_np')` argument adds suffixes to disambiguate columns with the same name, in this case `correct` and `related`. We use `mutate()` with `select()` to remove the `correct_` prefix from the 'correct' columns. We leave the `related_` prefix on the 'syntactically related' columns.

Our data is now fully preprocessed. Notice how the join sets cells to the value `NA` for subjects who were excluded for that particular test.

```{r echo=FALSE}
task_by_subj %>% pander(split.table = Inf)
```

<a name="cdi-diffs"></a>

## Comparing parents' ratings of their child's language ability

For our first analysis, we want to check that there were no differences between the children assigned to the Italian and English card groups. We do this using between-subjects t-tests of their parents' CDI ratings. Bayesian t-tests were introduced in the [Evidence worksheet](evidence.html).

```{r cdi}
library(BayesFactor, quietly=TRUE)
cdi_u_bf <- ttestBF(formula=cdi_u ~ cards, data = task_by_subj)
cdi_u_bf
cdi_s_bf <- ttestBF(formula=cdi_s ~ cards, data = task_by_subj)
cdi_s_bf
```

**Explanation of commands:**

First we load the `BayesFactor` package. Next, we run a t-test which compares CDI 'understands' (`cdi_u`) for the two card sets. We another t-test which compares CDI 'says' (`cdi_s`) for the two card sets.

**Explanation of output:**

Our Bayes factors (CDI understands = `r round(BayesFactor::extractBF(cdi_u_bf)$bf, 2)`; CDI says = `r round(BayesFactor::extractBF(cdi_s_bf)$bf, 2)`) do not provide conclusive evidence of differences between the children tested using the Italian and English cards, but they're low enough to give us some confidence that the two groups were matched on language ability.

<a name="table-descriptives"></a>

## Generating a table of statistics

Next we produce a table of descriptive statistics. This is described in more detail in the [Better tables worksheet](#better-tables.html). We start by calculating means and standard deviations for each WinG task by gender.

```{r descriptives}
task_by_subj_l <- task_by_subj %>%
  pivot_longer(cols = c(nc, np, pc, pp),
               names_to = 'task',
               values_to = 'correct')

descriptives <- task_by_subj_l %>%
  group_by(task, gender) %>%
  summarise(mean = mean(correct, na.rm = TRUE),
            sd = sd(correct, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate_if(is.numeric, round, 2)

descriptives <- descriptives %>%
  pivot_wider(names_from = gender, values_from = c(mean, sd)) %>%
  select(task, mean_male, sd_male, mean_female, sd_female)
```

**Explanation of commands:**

We convert `task_by_subj` to long format by using `pivot_longer()` on the correct score columns `nc`, `np`, `pc` and `pp`. We use this to calculate a mean and standard deviation by gender within task. When calculating these summary statistics, we need the argument `na.rm = TRUE` to ignore missing data for participants who were excluded from any of the tests. We round the results to 2 decimal places. We use `pivot_wider()` so that `descriptives` has a row for each task, and columns containing the mean and standard deviation by gender.

Our table of descriptive statistics now looks like this:

```{r echo=FALSE}
descriptives %>% pander()
```

We'd also like to test for gender differences on each of the tasks. We'll do this by running some t-tests and adding the results to our table.

```{r WinG-t-by-gender}
# t-test for accuracy by gender
t_gender <- function(df, group) {
  df <- drop_na(df) # remove data for excluded subjects
  bf <- ttestBF(formula=correct ~ gender, data = data.frame(df))
  extractBF(bf)
}

gender_bf <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(t_gender) %>%
  mutate(bf = round(bf,2)) %>%
  select(task, bf)

descriptives_t <- full_join(descriptives, gender_bf, by = 'task')
```

**Explanation of commands:**

We define the function `t_gender()` to run a between-subjects t-test comparing task accuracy (`correct`) by `gender`. Before running the test, we use `drop_na(df)` to remove rows for participants who were excluded for the task. The function returns the Bayes factor. We group our long format task data by task, and use `group_modify(t_gender)` to calculate the Bayes factor for the grouped data. We round the Bayes factors to 2 decimal places and then select the `task` and `bf` columns. We join this to our `descriptives` data frame.

Our data frame now looks like this:

```{r echo=FALSE}
descriptives_t %>% pander()
```

We tidy up the data and then present the data frame as a table.

```{r descriptives-presentation}
descriptives_t <- descriptives_t %>%
  unite("male", mean_male:sd_male, sep=' (') %>%
  unite("female", mean_female:sd_female, sep=' (') %>%
  mutate(male = paste0(male, ')'), female = paste0(female, ')'), task = factor(task),
         task = recode_factor(.$task, nc = 'Noun Comprehension', np = 'Noun Production',
                              pc = 'Predicate Comprehension', pp = 'Predicate Production'))

library(kableExtra)
descriptives_t %>% kable(
  col.names = c('Task', 'Male', 'Female', 'Bayes Factor')) %>%
  kable_styling()
```

**Explanation of commands:**

We use `unite()` to combine the male and female summary statistics into a single column with the standard deviation in parentheses. We rename `task` factor levels so that the reader will understand what they mean. Finally, we print the table as HTML, a format we could copy-paste into a word processor

**Explanation of output:**

The Bayes factors aren't conclusive, but all values are less than 1, suggesting it's most likely that task accuracy wasn't different for male and female children.

<a name="correlate-cdi-wing"></a>

## Exploring correlations between CDI and WinG

We would also like to know if parents' ratings of their child's ability using the CDI was related to their child's performance on the WinG tasks. We start by looking at correlations between CDI 'understands' comprehension scores on the noun and predicate tests.

```{r cdi_u_correlations}
cdi_u_nc_cor <- cor(method = 'spearman', task_by_subj$cdi_u, task_by_subj$nc, use="complete.obs")
cdi_u_nc_cor
cdi_u_nc_bf <- correlationBF(task_by_subj$cdi_u, task_by_subj$nc)
cdi_u_nc_bf

cdi_u_pc_cor <- cor(method = 'spearman', task_by_subj$cdi_u, task_by_subj$pc, use="complete.obs")
cdi_u_pc_cor
cdi_u_pc_bf <- correlationBF(task_by_subj$cdi_u, task_by_subj$pc)
cdi_u_pc_bf
```

**Explanation of commands:**

We use a Spearman correlation to test the relationship between `task_by_subj$cdi_u` and `task_by_subj$nc` (noun comprehension). This was introduced in the [More on relationships, part 2 worksheet](corr-extended.html). The option `use="complete.obs"` deletes any cases with a value of `NA`. Remember that we assigned the value `NA` to cells for subjects who were excluded from a test.
We also calculate a Bayes Factor to test the reliability of the relationship. We use similar commands to test the relationship between `cdi_u` and predicate comprehension (`pc`).

**Explanation of output:**

There doesn't appear to be a relationship between parents' CDI 'understands' scores and children's noun comprehension (_rs_ = `r round(cdi_u_nc_cor,2)`), although there is weak evidence against this conclusion (_BF_ = `r round(BayesFactor::extractBF(cdi_u_nc_bf)$bf,2)`). There seems to be a weak negative correlation between parents' CDI 'understands' scores and children's predicate comprehension (_rs_ = `r round(cdi_u_pc_cor,2)`), however, the Bayes factor of `r round(BayesFactor::extractBF(cdi_u_pc_bf)$bf,2)` means it's almost twice as likely that there isn't a relationship between these two variables as there is.

Now we look at correlations between parents' ratings of their child's ability to say words (CDI says), and the childrens' noun and predicate production scores.

```{r cdi_s_correlations}
cdi_s_np_cor <- cor(method = 'spearman', task_by_subj$cdi_s, task_by_subj$np, use="complete.obs")
cdi_s_np_cor
cdi_s_np_bf <- correlationBF(task_by_subj$cdi_s, task_by_subj$np)
cdi_s_np_bf

cdi_s_pp_cor <- cor(method = 'spearman', task_by_subj$cdi_s, task_by_subj$pp, use="complete.obs")
cdi_s_pp_cor
cdi_s_pp_bf <- correlationBF(task_by_subj$cdi_s, task_by_subj$pp)
cdi_s_pp_bf
```

**Explanation of commands:**

These calculations are identical to those above, except we are looking at the evidence for relationships between `cdi_u` and `task_by_subj$np` (noun production), and `task_by_subj$pp` (predicate production).

**Explanation of output:**

There is evidence of a strong, positive correlation between parents' CDI 'says' scores and children's noun production (_rs_ = `r round(cdi_s_np_cor,2)`, _BF_ = `r round(BayesFactor::extractBF(cdi_s_np_bf)$bf,2)`). There doesn't appear to be a relationship between CDI 'says' and children's predicate production (_rs_ = `r round(cdi_s_pp_cor,2)`, _BF_ = `r round(BayesFactor::extractBF(cdi_s_pp_bf)$bf,2)`).
 
<a name="plot-task"></a>

## Plotting WinG task scores (raincloud plot)

We'll create a raincloud plot to visualise the scores for the noun tasks. Raincloud plots were introduced in the [Better graphs worksheet](better-graphs.html).

First we define the function `gemo_flat_violin()` to produce the "cloud".

```{r flat-violin, echo=FALSE}
"%||%" <- function(a, b) {
  if (!is.null(a)) a else b
}

geom_flat_violin <- function(mapping = NULL, data = NULL, stat = "ydensity",
                             position = "dodge", trim = TRUE, scale = "area",
                             show.legend = NA, inherit.aes = TRUE, ...) {
  layer(
    data = data,
    mapping = mapping,
    stat = stat,
    geom = GeomFlatViolin,
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      trim = trim,
      scale = scale,
      ...
    )
  )
}

GeomFlatViolin <-
  ggproto("GeomFlatViolin", Geom,
    setup_data = function(data, params) {
      data$width <- data$width %||%
        params$width %||% (resolution(data$x, FALSE) * 0.9)

      # ymin, ymax, xmin, and xmax define the bounding rectangle for each group
      data %>%
        group_by(group) %>%
        mutate(
          ymin = min(y),
          ymax = max(y),
          xmin = x,
          xmax = x + width / 2
        )
    },

    draw_group = function(data, panel_scales, coord) {
      # Find the points for the line to go all the way around
      data <- transform(data,
        xminv = x,
        xmaxv = x + violinwidth * (xmax - x)
      )

      # Make sure it's sorted properly to draw the outline
      newdata <- rbind(
        plyr::arrange(transform(data, x = xminv), y),
        plyr::arrange(transform(data, x = xmaxv), -y)
      )

      # Close the polygon: set first and last point the same
      # Needed for coord_polar and such
      newdata <- rbind(newdata, newdata[1, ])

      ggplot2:::ggname("geom_flat_violin", GeomPolygon$draw_panel(newdata, panel_scales, coord))
    },

    draw_key = draw_key_polygon,

    default_aes = aes(
      weight = 1, colour = "grey20", fill = "white", size = 0.5,
      alpha = NA, linetype = "solid"
    ),

    required_aes = c("x", "y")
  )
```

Now we generate a plot for the noun tasks.

```{r noun-cloud}
noun_tasks <- task_by_subj_l %>%
    filter(task %in% (c('np','nc')))
noun_tasks$task <- recode_factor(noun_tasks$task, nc = 'Noun Comprehension', np = 'Noun Production')

x_dodge <- .1
palette <- 'Set1'

raincloud <- ggplot(noun_tasks, aes(x = task, y = correct, fill = cards)) +
  geom_flat_violin(aes(fill = cards), position = position_nudge(x = x_dodge, y = 0), adjust = 1.5, trim = FALSE, alpha = .5, colour = NA) +
  geom_point(aes(x = as.numeric(task)-.15, y = correct, colour = cards), position = position_jitter(width = .05), size = .25, shape = 20) +
  stat_summary(geom="errorbar", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), width = .1, fun.data=mean_cl_boot) +
  stat_summary(geom="point", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), fun.data=mean_cl_boot) +
  stat_summary(geom="line", position = position_nudge(x = x_dodge, y = 0), mapping = aes(group = cards, colour = cards), fun.data=mean_cl_boot, linetype = 3) +
  scale_colour_brewer(palette = palette) +
  scale_fill_brewer(palette = palette)

raincloud

ggsave(filename = "noun_cloud.pdf", plot = raincloud, units = "cm", width = 15, height = 10)
```

We generate a separate plot for the predicate tasks.

```{r predicate-cloud}
predicate_tasks <- task_by_subj_l %>%
    filter(task %in% (c('pc','pp')))
predicate_tasks$task <- recode_factor(predicate_tasks$task, nc = 'Noun Comprehension', np = 'Noun Production')

x_dodge <- .1
palette <- 'Set1'

raincloud <- ggplot(predicate_tasks, aes(x = task, y = correct, fill = cards)) +
  geom_flat_violin(aes(fill = cards), position = position_nudge(x = x_dodge, y = 0), adjust = 1.5, trim = FALSE, alpha = .5, colour = NA) +
  geom_point(aes(x = as.numeric(task)-.15, y = correct, colour = cards), position = position_jitter(width = .05), size = .25, shape = 20) +
  stat_summary(geom="errorbar", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), width = .1, fun.data=mean_cl_boot) +
  stat_summary(geom="point", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), fun.data=mean_cl_boot) +
  stat_summary(geom="line", position = position_nudge(x = x_dodge, y = 0), mapping = aes(group = cards, colour = cards), fun.data=mean_cl_boot, linetype = 3) +
  scale_colour_brewer(palette = palette) +
  scale_fill_brewer(palette = palette)

raincloud

ggsave(filename = "predicate_cloud.pdf", plot = raincloud, units = "cm", width = 15, height = 10)
```

The raincloud plot for the noun tasks indicates that the data was not normally distributed. Given the small sample sizes, a non-parametric test is most suited for analysing this data. Although the predicate data looks normally distributed, in this study, a Mann-Whitney U test was used to compare the scores for the Italian and English cards for both the noun and predicate tasks.

<a name="mann-whitney"></a>

## Comparing cards using Mann-Whitney U tests

The Mann-Whitney U is explained in the [Traditional non-parametric tests worksheets](non-parametric.html). We start by creating some the summary statistics which are used to interpret this test.

```{r cards-rank}
task_by_subj_l %>%
  group_by(task, cards) %>%
  drop_na() %>%
  summarise(n = n(),
            median = median(correct),
            mean_rank = mean(rank(correct)),
            sum_rank = sum(rank(correct)))
```

Now we run a test to compare the Italian and English cards for each WinG task.

```{r mann-whitney}
# independent 2-group Mann-Whitney U Test
mann_whitney <- function(df, group) {
  df <- drop_na(df) # remove excluded subjects
  n1 <- sum(as.integer(df$cards) == 1) # n in first group
  wilcox.test(correct ~ cards, df) %>%
    with(tibble(U = statistic,
              W = statistic + n1 * (n1 + 1) / 2,
              Z = qnorm(p.value / 2),
              p = p.value))
}

wing_mann_whitney <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(mann_whitney)

pander(wing_mann_whitney)
```

All _p_ values were > 0.05, suggesting there were no differences between the Italian and English cards on any of the tests.

<a name="compare-production"></a>

## Comparing semantically related production errors

We would like to know if there were any differences on these semantically related errors between the two sets of cards. A raincloud plot suggests there wasn't.

```{r nts-production-plot}
semantically_related <-
  select(task_by_subj, subj, cards, related_np, related_pp) %>%
  rename(np = 'related_np', pp = 'related_pp') %>%
  pivot_longer(cols = c(np, pp),
               names_to = 'task',
               values_to = 'correct') %>%
  mutate(task = recode_factor(.$task, np = 'Noun Production', pp = 'Predicate Production'))

related_cloud <- ggplot(semantically_related, aes(x = task, y = correct, fill = cards)) +
  geom_flat_violin(aes(fill = cards), position = position_nudge(x = x_dodge, y = 0), adjust = 1.5, trim = FALSE, alpha = .5, colour = NA) +
  geom_point(aes(x = as.numeric(task)-.15, y = correct, colour = cards), position = position_jitter(width = .05), size = .25, shape = 20) +
  stat_summary(geom="errorbar", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), width = .1, fun.data=mean_cl_boot) +
  stat_summary(geom="point", position = position_nudge(x = x_dodge, y = 0), mapping = aes(colour = cards), fun.data=mean_cl_boot) +
  stat_summary(geom="line", position = position_nudge(x = x_dodge, y = 0), mapping = aes(group = cards, colour = cards), fun.data=mean_cl_boot, linetype = 3) +
  scale_colour_brewer(palette = palette) +
  scale_fill_brewer(palette = palette)

related_cloud
```

Mann-Whitney U tests confirm this, with both _p_ values > 0.05.

```{r nts-production-mw}
production_mann_whitney <- semantically_related %>%
  group_by(task) %>%
  group_modify(mann_whitney)

pander(production_mann_whitney)
```

* "A Mann-Whitney-U test was run to determine if there were any statistically
significant differences in the number of non-target but semantically related errors
elicited by the English WinG cards and the Italian WinG cards in the noun production
sub-test. ... The number of non-target but semantically related errors was not statistically significantly different between the English cards (Mdn = 3) and the Italian cards (Mdn = 3), U = 65, z = .415, p = .713"

* "A Mann-Whitney-U test was run to determine if there were differences in the
number of non-target but semantically related errors elicited by the English WinG
cards and the Italian WinG cards in the predicate production sub-test. Distributions of
the scores for the English cards and the Italian cards were not similar, as assessed
by visual inspection. The number of correct responses for the English cards (mean
rank = 8.60) and the Italian cards (mean rank = 10.62) were not statistically
significantly different, U = 31, z = -.821, p = .460"

<a name="table-cards"></a>

## Comparing individual cards

We would like to know if there were any accuracy differences between the individual Italian and English cards in each task. We can summarise this data as a table of t-tests. Running a t-test where both groups have identical scores produces an error, so we remove these cards from our tests. For noun comprehension, this was the case for the words 'apple', 'cow', 'penguin','hat', and 'motorbike'.

```{r t-nc}
nc_long <- nc %>%
  exclude %>%
  select(-correct) %>%
  pivot_longer(cols=(mountain:wellyboots),
               names_to = 'word',
               values_to = 'answer')

correct_word <- function(df, group) {
  df %>% select(answer) %>% summarise(correct = colSums(. == 'C' | . == 'C*'))
}
nc_long_correct <- nc_long %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

t_words <- function(df, group) {
  bf <- ttestBF(formula=correct ~ cards, data = data.frame(df))
  extractBF(bf)
}

nc_bf <- nc_long_correct %>%
  filter(! word %in% c('apple', 'cow', 'penguin','hat','motorbike')) %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

nc_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

We generate similar tables for noun production ...

```{r t-np}
np_long <- np %>%
  exclude() %>%
  select(-correct) %>%
  pivot_longer(cols=(beach:gloves),
               names_to = 'word',
               values_to = 'answer')

np_long_correct <- np_long %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

np_bf <- np_long_correct %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

np_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

... predicate comprehension ...

```{r t-pc}
pc_long <- pc %>%
  exclude() %>%
  select(-correct) %>%
  pivot_longer(cols=(big:pulling),
               names_to = 'word',
               values_to = 'answer')

pc_long_correct <- pc_long %>%
  filter(! word %in% c('climbing','waving', 'running', 'drinking', 'walking')) %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

pc_bf <- pc_long_correct %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

pc_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

... and predicate production.

```{r t-pp}
pp_long <- pp %>%
  exclude() %>%
  select(-correct) %>%
  pivot_longer(cols=(small:pushing),
               names_to = 'word',
               values_to = 'answer')

pp_long_correct <- pp_long %>%
    filter(! word %in% c('far-apart', 'heavy', 'in-front')) %>%
  group_by(word, subj, cards) %>%
  group_modify(correct_word)

pp_bf <- pp_long_correct %>%
  group_by(word) %>%
  group_modify(t_words) %>%
  mutate(bf = round(bf,2)) %>%
  select(word, bf)

pp_bf %>% kable(
  col.names = c('Word', 'Bayes Factor')) %>%
  kable_styling()
```

The only Bayes factor which provided evidence of a difference between the cards was `bag` on the noun production task.

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 

