---
title: "Reliability of Targets in a Picture Naming Task"
author: "Allegra Cattani, Adele Conn, Paul Sharpe and Andy Wills"
output:
  html_document:
    highlight: pygment
---

```{r setup, include=FALSE}
## DEVELOPERS: Uncomment one option, as appropriate
## Show only commands.
## knitr::opts_chunk$set(echo = TRUE, message = FALSE, results='hide', fig.keep = 'none', comment=NA)
## Show commands and output.
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache = TRUE)
library(pander)
# Line numbering guide
# https://blog.atusy.net/submodules/rmd-line-num/index.html
```

## Before you start

This is an advanced worksheet, which assumes you have completed the [Absolute Beginners' Guide to R](https://ajwills72.github.io/rminr/#beginners) course, the [Research Methods in Practice (Quantitative section)](https://benwhalley.github.io/rmip/overview-quantitative.html) course, and the [Intermediate Guide to R](https://ajwills72.github.io/rminr/#rmip) course. 

## Contents

- [Introduction](#intro)
- [Loading data](#data)
- [Preprocessing demographics data](#preproc-demographics)
- [Preprocessing WinG noun comprehension data](#preproc-nc)
- [Applying exclusion criteria](#exclude)
- [Comparing parents' ratings of their child's language ability](#cdi-diffs)
- [Comparing gender on WinG accuracy](#wing-gender)
- [Exploring correlations between CDI and WinG](#correlate-cdi-wing)
- [Plotting WinG accuracy by card set](#plot-task)
- [Comparing WinG accuracy by card set](#mann-whitney)
- [Comparing semantically related production errors by card set](#compare-production)

<a name="intro"></a>

## Introduction

This worksheet describes a full analysis pipeline for an undergraduate student dissertation on childrenâ€™s language development. This study was an experiment which evaluated the _Words in Game_ (WinG) test. WinG consists of a set of picture cards which are used in four tests: noun comprehension, noun production, predicate comprehension, and predicate production. The Italian and English versions of the WinG cards use different pictures to depict the associated words.

An earlier study found a difference between the English and Italian cards, for adults' ratings of how well each picture represented the underlying construct. In this study, the researchers hypothesised that this difference would influence children's WinG task scores, depending on which set of cards they were tested with. The experiment compared WinG performance of English-speaking children, aged approximately 30 months, tested with either the Italian or English cards. Therefore, this was a 4 (WinG task) x 2 (cards) mixed design.

<a name="data"></a>

## Loading data

Open the `rminr-data` project we used [previously](preproc.html#load).

Ensure you have the latest files by asking git to "`pull`" the repository. Select the `Git` tab, which is located in the row of tabs which includes the `Environment` tab. Click the `Pull` button with a downward pointing arrow. A window will open showing the files which have been pulled from the repository. Close the `Git pull` window. The `case-studies` folder should contain a folder named `allegra-cattani`.

Next, create a new, empty R script and save it in the `rminr-data` folder as `wing.R`. Put all the commands from this worksheet into this file, and run them from there. Save your script regularly.

We start by reading the data.

```{r load, message=FALSE}
rm(list = ls()) # clear the environment
library(tidyverse)

# read data
demographics <- read_csv('case-studies/allegra-cattani/demographics.csv')
nc <- read_csv('case-studies/allegra-cattani/noun_comprehension.csv')
np <- read_csv('case-studies/allegra-cattani/noun_production.csv')
pc <- read_csv('case-studies/allegra-cattani/predicate_comprehension.csv')
pp <- read_csv('case-studies/allegra-cattani/predicate_production.csv')
```

**Explanation of commands:**

1. We clear the workspace, and load the `tidyverse`package.
1. Data for each of the WinG tasks is stored in its own CSV file. There is also file containing 'demographics' data. We read each file into its own data frame.

<a name="preproc-demographics"></a>

## Preprocessing demographics data

Next we preprocess the demographics data.

```{r demographics, message=FALSE}
# preprocess demographics data
demographics <- demographics %>%
  mutate(subj = factor(subj), Gender = factor(Gender)) %>%
  select(subj, Gender, CDI_U, CDI_S)
```

**Explanation of commands:**

We convert some columns to  [factors](anova1.html#anovaWS), and then we `select` the `Gender`, `CDI_U` and `CDI_S` columns. These latter two columns contain scores for the Communicative Development Inventory (CDI). The CDI is a list of 100 words with columns 'understands' and 'says'. For each word, the child's parent placed a tick in these columns if they thought their child could understand and/or say the word. The scores for 'understands' (`CDI_U`) and 'says' (`CDI_S`) are a total of the ticked boxes in the two columns. 

Our `demographics` data frame now contains the child's subject number, their gender and their parents' CDI scores:

```{r echo=FALSE}
demographics %>% head(3) %>% pander()
```

<a name="preproc-nc"></a>

## Preprocessing WinG noun comprehension data

Next we preprocess the WinG data.

```{r nc}
# preprocess noun comprehension data
nc <- nc %>% mutate(subj = factor(subj), Cards = factor(Cards)) 
```

**Explanation of commands:**

The `cards` column indicates whether the child was tested with the Italian or English cards, so we convert it to a factor along with `subj`. We select these columns and the columns containing rating data for the words used in the the noun comprehension task. These are in the column range `mountain:wellyboots`.

Here are the first few rows of `nc`:

```{r echo=FALSE}
nc %>% head(3) %>% pander(split.table = Inf)
```
<a name="exclude"></a>

## Applying exclusion criteria

Before we go any further, we need to apply some exclusion criteria to our data. When the experimenters test children with WinG, they use a textual coding scheme to record each child's responses. The codes we're interested in here are:

* `N/A` (not to be confused with the `R` data type `NA`) - researchers used this code to indicate that the task was interrupted for some reason (e.g. the child began crying).
* `C` or `C*` - `C` indicates that the child responded correctly for the picture on the card, `C*` indicates that the response was a correct synonym. In this experiment, both of these values are considered correct responses.
* `NTS` - This stands for "non-target but semantically related". This code is used in the noun and predicate production tests, for example if the picture on the card was a house but the child said "hut".

In this experiment, the exclusion criteria were as follows:

1. Any subject with 'N/A' for one of the first 17 words in a task. This criterion excludes children with insufficient data for analysis.
1. Any subject in the remaining data whose accuracy was less than two standard deviations below the mean. This is a more general criterion to exlude subjects who had especially low scores for various reasons.

### Defining a function

Although these exclusion criteria are quite easy to describe, writing the code to apply them is a bit more complex. Furthermore, we need to apply these criteria separately for each of the four WinG tasks. Writing the same code four times would make our program harder to read and is error prone. If we were to discover that we'd made a mistake, we would have to fix it in four different places. Fortunately, we can just write the code once and then reuse it. We do this using a function.

Functions are reusable lines of code which perform a particular function (hence the name), and are one of the most powerful features of programming languages such as R. At this point you will be familiar with _using_ functions. For example `rm()` is a function for removing objects from the R environment, and is part of the base `R` language. The `rm()` function is reusable in the sense that you can use the same function to do slightly different things. For example, `rm('foo')` would remove the single object `foo` from your environment. The string `foo` is called an _argument_ and is the data processed by `rm()`. The same function can also remove multiple variables provided in a list, so calling `rm(ls())`, first calls `ls()` which lists all objects in your environment, and passes the results as an argument to `rm()` which removes them, thereby cleaning your environment of all objects. Similarly, you've used `read_csv()`, with different filename arguments to read different data files. The `read_csv()` function is part of the `readr` package which is loaded when you call `library(tidyverse)` (`library()` is another base `R` function). 

Removing objects from the environment is such a common task that it's a function included with `R`. Reading files is almost as common, which is why it is part of the `readr` package that you load with `library(tidyverse)`. However, `R` also includes a special function called `function()`, which you can use to define your own functions. This is not particularly complicated, but requires a little explanation if you've never encountered the idea before (you can find more details in [the R manual](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Writing-your-own-functions)). We'll explain the steps by writing a function called `exclude()`, allowing us to apply our exclusion criteria for each of the four WinG tests.

```{r exclude}
# apply WinG exclusion criteria
exclude <- function(df) {
  # exclude if N/A in item 17 or lower
  logical_matrix <- df == 'N/A'
  q17 <- logical_matrix %>%
    which(arr.ind = TRUE) %>%
    data.frame() %>%
    group_by(row) %>%
    summarise(min = min(col)) %>%
    mutate(subj = factor(row)) %>%
    select(subj, min)
  q17 <- left_join(df, q17, by='subj') %>%
    replace_na(list(min = 20))
  q17 <- q17 %>% filter(min > 17) %>% select(-min)
  
  # calculate total correct and semantically related
  q17 <- q17 %>% mutate(correct = rowSums(. == 'C' | . == 'C*'),
                        related = rowSums(. == 'NTS'))
  
  # exclude participants with scores < 2 sd below the mean
  q17 <- q17 %>% filter(correct < mean(correct) + 2 * sd(correct))

  return(q17)
}
```

**Explanation of commands:**

1. Our `exclude()` function accepts a data frame argument `df`, containing the data for a single WinG task.
1. `logical_matrix <- df == 'N/A'` converts `df` to a matrix containing the value `TRUE` where a cell in `df` contained the string `N/A`, or `FALSE` otherwise.
1. `which(arr.ind = TRUE)` creates a matrix with columns `row` (this will be the same as our subject number) and `col` containing the row and column number of any cells with the value `TRUE` (the cells that originally contained `N/A`).
1. We convert this to a data frame which we group by `row`.
1. `summarise(min = min(col))` summarises the grouped data by creating a single row containing the lowest column number that contained `N/A`.
1. `mutate(subj = factor(row)) %>% select(subj, min)` creates a factor `subj` from `row`, and selects this along with the column number of the first column that contained `N/A`. We have now converted our original data into a data from with rows for any subjects who had at least one `N/A` and the number of the first word where the task was interrupted.
1. `q17 <- left_join(df, q17, by='subj')` joins this data frame to the data frame we passed to `exclude()`. For rows where there is no matching `subj`, `min` gets the value `NA`. `replace_na(list(min = 20))` converts these `NA`s to the value `20`, indicating the child provided an answer for all words.
1. To complete this exclusion criterion, `q17 <- q17 %>% filter(min > 17) %>% select(-min)` selects only children who provided answers beyond word 17, and then removes the `min` column, as it's no longer needed.
1. Next we calculate accuracy scores for each subject across all words. `q17 <- q17 %>% mutate(correct = rowSums(. == 'C' | . == 'C*'))` creates a new column `correct` with a value for each row which is the sum of the number of columns containing `C` or `C*` (`|` means 'or'). Cells containing `C` indicate that the child responded correctly for the picture on the card for the the word in this column. Cells containing `C*` indicates that the response was a correct synonym. In this experiment, both of these values are considered correct responses. Similarly, we calculate the total number columns containing `NTS` and store this in the column `related`. We'll use this value in a later analysis.
1. Now we apply the second exclusion criterion. `q17 %>% filter(correct < mean(correct) + 2 * sd(correct))` calculates the mean and standard deviation for `correct`, and then filters any subjects whose value for `correct` was less than two standard deviations below the mean.
1. Finally, we return the data frame without rows meeting our two exclusion critera.

Now we can use `exclude()` to apply the exclusion criteria to our data.

```{r exclude-nc}
nc_by_subj <- nc %>% exclude() %>% select(subj, correct, related)
```

**Explanation of commands:**

We pipe `nc` into `exclude()`, which applies our exclusion criteria. As a side-effect, it also calculates the `correct` and `related` scores for each subject.

Looking at `nc_by_subj`, we can see that subject `18` was excluded from the noun comprehension task. The `related` scores are all 0 because this column is only relevant for the word production tasks.

```{r, echo=FALSE}
nc_by_subj %>% pander()
```

We repeat these steps to apply exclusion critera for the noun production, predicate comprehension and predicate production tasks.

```{r np-pc-pp}
np <- np %>% mutate(subj = factor(subj), Cards = factor(Cards))
np_by_subj <- np %>% exclude() %>% select(subj, correct, related)

pc <- pc %>% mutate(subj = factor(subj), Cards = factor(Cards))
pc_by_subj <- pc %>% exclude() %>% select(subj, correct, related)

pp <- pp %>% mutate(subj = factor(subj), Cards = factor(Cards))
pp_by_subj <- pp %>% exclude() %>% select(subj, correct, related)
```
**Explanation of commands:**

This code is the same as for the noun comprehension task, apart the range of columns we `select()` for each task. For noun production, the scores are in `beach:gloves`, for predication comprehension they're in `big:pulling`, and for predication production they're in `small:pushing`.

Having preprocessed the demographics data and applied our exclusions to the WinG data, we now join these data frames together in preparation for calculating our descriptive statistics.

```{r join}
# join data
task_by_subj <- nc %>% select(subj, cards) %>%
  full_join(., demographics, by='subj')
```

**Explanation of commands:**

We start by creating a data frame containing `subj` and `cards` for all participants, and joining it with the `demographics` data frame. We use `nc` to create the cards data, as this was a data frame _before_ we applied any exclusion critera. If we didn't do this, in the next stage we would end up with `NA` values in `cards` for excluded participants.

Here's `task_by_subj`:

```{r echo=FALSE}
task_by_subj %>% pander()
```

We join `task_by_subj` to the other WinG task data frames:

```{r join-more}
task_by_subj <- task_by_subj %>%
  full_join(., nc_by_subj, by='subj') %>%
  full_join(., np_by_subj, by='subj', suffix = c('_nc','_np')) %>%
  full_join(., pc_by_subj, by='subj') %>%
  full_join(., pp_by_subj, by='subj', suffix = c('_pc', '_pp')) %>%
  mutate(nc = correct_nc, np = correct_np, pc = correct_pc, pp = correct_pp) %>%
  select(subj, Gender, cards, nc, np, pc, pp, CDI_U, CDI_S,
         related_nc, related_np, related_pc, related_pp)
```

**Explanation of commands:**

Each `full_join()`, joins an additional data frame by matching values in `subj`. The `suffix = c('_nc','_np')` argument adds suffixes to disambiguate columns with the same name, in this case `correct` and `related`. We use `mutate()` with `select()` to remove the `correct_` prefix from the 'correct' columns. We leave the `related_` prefix on the 'syntactically related' columns.

Our data is now fully preprocessed. Notice how the join sets cells to the value `NA` for subjects who were excluded for that particular task.

```{r echo=FALSE}
task_by_subj %>% pander(split.table = Inf)
```

<a name="cdi-diffs"></a>

## Comparing parents' ratings of their child's language ability

For our first analysis, we want to check that there were no differences in language ability between the children assigned to the Italian and English card groups. We do this using between-subjects t-tests of their parents' CDI ratings. Bayesian t-tests were introduced in the [Evidence worksheet](evidence.html).

```{r cdi}
# compare children's language ability using CDI
library(BayesFactor, quietly=TRUE)
cdi_u_bf <- ttestBF(formula=CDI_U ~ cards, data = task_by_subj)
cdi_u_bf
cdi_s_bf <- ttestBF(formula=CDI_S ~ cards, data = task_by_subj)
cdi_s_bf
```

**Explanation of commands:**

First we load the `BayesFactor` package. Next, we run a t-test which compares CDI 'understands' (`CDI_U`) for the two card sets. We another t-test which compares CDI 'says' (`CDI_S`) for the two card sets.

**Explanation of output:**

Here, we're hoping to find evidence for the null hypothesis i.e. no differences in the means for the two groups. Our Bayes factors (CDI understands = `r round(BayesFactor::extractBF(cdi_u_bf)$bf, 2)`; CDI says = `r round(BayesFactor::extractBF(cdi_s_bf)$bf, 2)`) indicate that it's 2-2.5 times more likely that there's no difference, than there is a difference in language ability between the children tested using the Italian and English cards.

<a name="wing-gender"></a>

## Comparing gender on WinG accuracy

We would also like to check that there were no gender differences on any of the tests. We'll create a table of descriptive statistics with four rows corresponding to the WinG task, and columns containing acurracy means and standard deviations, for male and female children. This table is described in detail in the [Better tables worksheet](#better-tables.html). It's convenient to see differences between groups next to the descriptive statistics. We'll do this by adding a column to our table. Each cell will contain the result of a t-test comparing male and female accurcy, for the WinG task in the table row.

We start by calculating means and standard deviations for each WinG task by gender.

```{r descriptives}
# table of descriptives and t-tests for WinG by gender
task_by_subj_l <- task_by_subj %>%
  pivot_longer(cols = c(nc, np, pc, pp),
               names_to = 'task',
               values_to = 'correct')

descript <- task_by_subj_l %>%
  group_by(task, Gender) %>%
  summarise(mean = mean(correct, na.rm = TRUE), sd = sd(correct, na.rm = TRUE))
```

**Explanation of commands:**

We convert `task_by_subj` to long format by using `pivot_longer()` on the correct score columns `nc`, `np`, `pc` and `pp`. We use this to calculate a mean and standard deviation by gender within task. When calculating these summary statistics, we need the argument `na.rm = TRUE` to ignore missing data for participants who were excluded from any of the tests. We round the results to 2 decimal places. We use `pivot_wider()` so that `descriptives` has a row for each task, and columns containing the mean and standard deviation by gender.

Our table of descriptive statistics now looks like this:

```{r echo=FALSE}
descript %>% pander()
```

Next we do some preparation for displaying our table in APA format.

```{r apa}
descript_table <- descript %>%
  pivot_wider(names_from = Gender, values_from = c(mean, sd))
descript_table <- descript_table %>% select(task, mean_Female, sd_Female, mean_Male, sd_Male) 
descript_table %>% pander()
```

**Explanation of commands:**

We widen the table, using the `pivot_wider` command we used previously in the [within-subject differences](anova1.html#pivot) worksheet. We use `select` to order the columns so that the means are next to their associated standard deviations.

Now we run some Bayesian t-tests to test for gender differences on each of the tasks. Between subjects t-tests were covered in the [Evidence worksheet](evidence.html).

```{r WinG-t-by-gender, class.source = 'numberLines lineAnchors'}
# t-test for accuracy by gender
t_gender <- function(df, group) {
  df <- drop_na(df) # remove data for excluded subjects
  bf <- ttestBF(formula=correct ~ Gender, data = data.frame(df))
  extractBF(bf)
}
```

**Explanation of commands:**

We'll be doing the same t-test four times, once for the data from each task. Like the code for applying exclusion criteria that you saw earlier, this makes it a good candidate for a function. Line 1 begins the definition of the `t_gender()` function. The function will run the t-test using the data it assigns to the variable `df`. We won't use the `group` argument, but we'll explain why it's there when we call the function. Line 2 uses `drop_na()` to remove rows for participants who were excluded for the task in `df`. Recall that we set these values to `NA`. Line 3 uses `ttestBF()` to run a Bayesian t-test which compares task accuracy (`correct`) for each `gender`. Line 4 returns the Bayes factor.

Now we can use the function to run the four t-tests.

```{r do-t-by-gender, class.source = 'numberLines lineAnchors'}
gender_bf <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(t_gender) %>%
  select(task, bf)
gender_bf %>% pander()
```

Line 3 calls `t_gender()` for each of the groups created by `group_by(task)` in line 2. `group_modify()` calls a function for each group of a grouped data frame. It always passes the function two values. The first is a data frame containing the subset of rows for the group. The second is the name of the group. The function must accept two arguments, which explains why we defined `group` in `t_gender()`, even though we didn't use it. The data supplied by `group_modify()` is replaced with the data frame returned by the function it calls. The column which defines the groups is preserved. So here, we've use `t_gender()`, to replace the data for each test, with a data frame containing the Bayes factor for the associated t-test.

Finally, we join this to our `descriptives` data frame, using the `task` values to match up the rows.
 
```{r join-bf}
descript_table <- full_join(descript_table, gender_bf, by = 'task')
descript_table %>% pander()
```

We can now tidy the table up ready for printing.

```{r mean-labels, class.source = 'numberLines lineAnchors'}
task_names <- c(
  nc = 'Noun Comprehension',
  np = 'Noun Production',
  pc = 'Predicate Comprehension',
  pp = 'Predicate Production'  
)
descript$task <- descript$task %>% recode(!!!task_names)

colnames(descript_table) <-
  c("Task", "Female (M)", "Female (SD)", "Male (M)", "Male (SD)", "BF")
```

**Explanation of commands:**

Lines 1-7 use the approach for renaming factor levels that was introduced in the [cleaning up questionnaire data worksheet](https://benwhalley.github.io/rmip/worksheet-recoding.html). The last 2 lines give the columns more meaningful headings.

Now we can print our finished table.

```{r descriptives-presentation}
library(kableExtra)
descript_table %>% kable(digits=2) %>% kable_styling()
```

**Explanation of commands:**

We load the `kableExtra` package and pipe our data into `kable()`. The `digits=2` part ensures that every number is reported to two decimal places. `kable_styling()` prints the table to the Viewer window in RStudio. This table could be included in a reporty by copy-pasting into a word processor, and then styled according to APA guidelines.

**Explanation of output:**

The Bayes factors mean that it's about twice as likely that there are no gender differences than that there are, for noun comprehension and predicate production. For noun production, it's about 1.5 times as like that there isn't a gender difference, than that there is. For predicate comprehension, the evidence for or against a gender difference is about equal.

<a name="correlate-cdi-wing"></a>

## Exploring correlations between CDI and WinG

We would also like to know if parents' ratings of their child's ability using the CDI was related to their child's performance on the WinG tasks. We start by looking at correlations between CDI 'understands' comprehension scores on the noun and predicate tests.

```{r cdi_u_correlations}
# correlations between CDI and WinG
cdi_u_nc_cor <- cor(method = 'spearman', task_by_subj$CDI_U,
                    task_by_subj$nc, use="complete.obs")
cdi_u_nc_cor
cdi_u_nc_bf <- correlationBF(task_by_subj$CDI_U, task_by_subj$nc)
cdi_u_nc_bf

cdi_u_pc_cor <- cor(method = 'spearman', task_by_subj$CDI_U,
                    task_by_subj$pc, use="complete.obs")
cdi_u_pc_cor
cdi_u_pc_bf <- correlationBF(task_by_subj$CDI_U, task_by_subj$pc)
cdi_u_pc_bf
```

**Explanation of commands:**

We use a Spearman correlation to test the relationship between `task_by_subj$CDI_U` and `task_by_subj$nc` (noun comprehension). This was introduced in the [More on relationships, part 2 worksheet](corr-extended.html). The option `use="complete.obs"` deletes any cases with a value of `NA`. (Remember that we assigned the value `NA` to cells for subjects who were excluded from a test.) We also calculate a Bayes Factor to test the reliability of the relationship. We use similar commands to test the relationship between `CDI_U` and predicate comprehension (`pc`).

**Explanation of output:**

There doesn't appear to be a relationship between parents' CDI 'understands' scores and children's noun comprehension (_rs_ = `r round(cdi_u_nc_cor,2)`), although there is weak evidence against this conclusion (_BF_ = `r round(BayesFactor::extractBF(cdi_u_nc_bf)$bf,2)`). There seems to be a weak negative correlation between parents' CDI 'understands' scores and children's predicate comprehension (_rs_ = `r round(cdi_u_pc_cor,2)`), however, the Bayes factor of `r round(BayesFactor::extractBF(cdi_u_pc_bf)$bf,2)` means it's almost twice as likely that there isn't a relationship between these two variables, than that there is.

Now we look at correlations between parents' ratings of their child's ability to say words (CDI says), and the childrens' noun and predicate production scores.

```{r cdi_s_correlations}
cdi_s_np_cor <- cor(method = 'spearman', task_by_subj$CDI_S,
                    task_by_subj$np, use="complete.obs")
cdi_s_np_cor
cdi_s_np_bf <- correlationBF(task_by_subj$CDI_S, task_by_subj$np)
cdi_s_np_bf

cdi_s_pp_cor <- cor(method = 'spearman', task_by_subj$CDI_S,
                    task_by_subj$pp, use="complete.obs")
cdi_s_pp_cor
cdi_s_pp_bf <- correlationBF(task_by_subj$CDI_S, task_by_subj$pp)
cdi_s_pp_bf
```

**Explanation of commands:**

These calculations are identical to those above, except we are looking at the evidence for relationships between `cdi_u` and `task_by_subj$np` (noun production), and `task_by_subj$pp` (predicate production).

**Explanation of output:**

There is evidence of a strong, positive correlation between parents' CDI 'says' scores and children's noun production (_rs_ = `r round(cdi_s_np_cor,2)`, _BF_ = `r round(BayesFactor::extractBF(cdi_s_np_bf)$bf,2)`). There doesn't appear to be a relationship between CDI 'says' and children's predicate production (_rs_ = `r round(cdi_s_pp_cor,2)`, _BF_ = `r round(BayesFactor::extractBF(cdi_s_pp_bf)$bf,2)`).

In summary, noun production was the only task in which parents' ratings of their childrens' ability matched the childrens' accuracy.

<a name="plot-task"></a>

## Plotting WinG accuracy by card set

We're now ready to test our main hypothesis, which predicts that there will be a difference WinG task scores, depending on which set of cards the children were tested with. We'll start by creating plots to show the distribution of scores for the two card sets on the WinG tasks.

```{r noun-cloud, class.source = 'numberLines lineAnchors'}
# plot WinG accuracy by card set
task_by_subj_l$task <- task_by_subj_l$task %>% recode(!!!task_names)
library(see)
task_by_subj_l %>% ggplot(aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.7, size=0) +
  scale_fill_grey() +
  xlab('WinG Task') + ylab('Accuracy (max = 20)')
```

**Explanation of commands:**

Line 2 recodes the `task` factor levels, to make them more meaningful on the plot's x axis. Line 3 loads the `see` package which provides the `half_violin()` function. Line 4 defines the x axis of our plot to be the WinG tasks, the y axis to be task accuracy (1-20), and to use the `cards` factor for the fill colour. Line 5 creates a "half violin" plot. As the name suggests, this shows one half of a [violin plot](https://en.wikipedia.org/wiki/Violin_plot). `position = position_identity()` plots the two distributions on top of each other, making it easy to see how much they overlap. `alpha=0.7` changes to transparency, again to help us see the overlapping area. `size=0` removes the outline around the distributions. Line 6 uses a grey palette for filling in the two distributions. Line 7 gives our axes meaningful labels.

**Explanation of output:**

This plot gives a visual indication of whether there were differences between the Italian and English cards on each of the tests. Given the extensive overlap in scores between the card sets, this seems unlikely. The plot also shows that the data were slightly skewed on the noun tasks due to some low scores.

<a name="mann-whitney"></a>

## Comparing WinG accuracy by card set

Although the plot suggests that the predicate data was normally distributed, given the small sample sizes, a non-parametric tests was considered most appropriate for analysing this data. We'll use Mann-Whitney U tests to compare each task score for the Italian and English cards. The Mann-Whitney U test is explained in more detail in the [Traditional non-parametric tests worksheet](non-parametric.html).

We start by creating some summary statistics.

```{r cards-rank, class.source = 'numberLines lineAnchors'}
# compare WinG accuracy by card set
task_by_subj_rank <- task_by_subj_l %>%
  group_by(task) %>%
  mutate(rank = rank(correct))

task_summary <- task_by_subj_rank %>%
  group_by(task, cards) %>%
  drop_na('correct') %>%
  summarise(n = n(),
            median = median(correct),
            mean_rank = mean(rank),
            sum_rank = sum(rank))
task_summary
```

**Explanation of commands:**

Lines 1-3 rank the accuracy scores for each of the four tests. Line 6 groups the data by `cards` within `task`. Line 7 removes any rows where `correct` contains the value `NA`. These were the children we excluded earlier. Lines 8-11 calculate, for each group, the number of children, the median, and the mean and sum of the ranked items.

Now we run Mann-Whitney tests to make a direct comparison between the Italian and English cards for each WinG task.

```{r mann-whitney, class.source = 'numberLines lineAnchors', warning=FALSE}
# independent 2-group Mann-Whitney U Test
mann_whitney <- function(df, group) {
  df <- df %>% drop_na('correct') # remove excluded subjects
  wilcox.test(correct ~ cards, df) %>%
    with(data.frame(U = statistic, p = round(p.value, 2)))
}

wing_mann_whitney <- task_by_subj_l %>%
  group_by(task) %>%
  group_modify(mann_whitney)
wing_mann_whitney
```

**Explanation of commands:**

The Mann-Whitney U test is a non-parametric equivalent of a t-test. It's explained in detail in the [Traditional non-parametric tests worksheet](non-parametric.html). Lines 2-6 define a function called `mann_whitney` to run a Mann-Whitney test comparing the `correct` column against the factor `cards`. The function returns the values for `U` and `p` as a data frame. 

Lines 8-10 group our data by `task`, then use `group_modify(mann_whitney)` to replace the data for each group with the Mann-Whitney test results. Line 11 displays the results.

**Explanation of output:**

As this is a traditional statistical test, the p-value indicates whether there was a significant difference between the cards on any of the tasks. All _p_ values were > 0.05, suggesting there were no differences, contrary to our hypothesis. (Note that we've removed the `cannot compute exact p-value with ties` warnings to make the output easier to read.)

<a name="compare-production"></a>

## Comparing semantically related production errors by card set

We would also like to know if there were any differences for the semantically related errors between the two sets of cards.

Again, we start with a plot.

```{r nts-production-plot, class.source = 'numberLines lineAnchors'}
# compare semantically related production errors by card set
semantically_related <- task_by_subj %>%
  select(subj, cards, related_np, related_pp) %>%
  rename(np = related_np, pp = related_pp) %>%
  pivot_longer(cols = c(np, pp),
               names_to = 'task',
               values_to = 'correct')
semantically_related$task <- semantically_related$task %>% recode(!!!task_names)

ggplot(semantically_related, aes(x = task, y = correct, fill = cards)) +
  geom_violinhalf(position = position_identity(), alpha=0.7, size=0) +
  scale_fill_grey() +
  xlab('WinG Task') + ylab('Semantically related errors (max = 20)')
```

**Explanation of commands:**

Recall that 'semantically related' only applies to the production tasks, so we select just the data for these tasks in line 2. Line 3 renames the task columns. This will allow us to recode the factors using `task_names` that we defined earlier. Lines 4-6 convert the semantically related error scores from wide to long format. Notice that we use `values_to` to move these scores to a column named `correct`. This will allow us to reuse the `mann_whitney` function later. Line 7 gives the factor levels meaningful names. Lines 9-12 generate half violin plots for the production tasks, using the same approach we took for the task accuracy plot.

**Explanation of output:**

The extensive overlap in the distribution of semantically scores suggests there were no differences between the Italian and English cards for semantically related errors on the production tasks.

We generate some summary statistics, and run some Mann-Whitney U tests to test this.

```{r nts-production-mw, message=FALSE, warning=FALSE}
nts_rank <- semantically_related %>%
  group_by(task) %>%
  mutate(rank = rank(correct))

nts_summary <- nts_rank %>%
  group_by(task, cards) %>%
  drop_na('correct') %>%
  summarise(n = n(),
            median = median(correct),
            mean_rank = mean(rank),
            sum_rank = sum(rank))
nts_summary

production_mann_whitney <- semantically_related %>%
  group_by(task) %>%
  group_modify(mann_whitney)
production_mann_whitney
```

**Explanation of commands:**

These commands are virtually identical to those we used to for the accuracy scores.

**Explanation of output:**

The group difference for noun production was larger than for predicate production. However, the _p_ values for both tests are > 0.05, which is evidence against a difference between the card sets for semantically related errors on the production tasks. Again, this is contrary to our hypothesis.

## Conclusion

In summary, these data suggest that WinG scores should be comparable, regardless of whether the English or Italian picture cards are used. It also shows that adults' ratings of how the pictures map to the underlying concepts are quite different to children's.

___

This material is distributed under a [Creative Commons](https://creativecommons.org/) licence. CC-BY-SA 4.0. 

